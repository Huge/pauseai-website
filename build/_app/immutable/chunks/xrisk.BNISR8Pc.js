import {
	s as Cs,
	a as xa,
	e as xs,
	c as i,
	t as u,
	b as p,
	f as r,
	h as g,
	i as h,
	j as a,
	d as c,
	g as d,
	k as E,
	l as s,
	m as l
} from './scheduler.D9JQr37X.js'
import {
	S as ks,
	i as Ts,
	c as $,
	b as w,
	m as v,
	a as _,
	t as b,
	d as y
} from './index.D-WnFt3a.js'
import { g as Ps, a as Is } from './a.svelte_svelte_type_style_lang.DfavE63L.js'
import { M as Ls } from './mdsvex.Bi9EMyuJ.js'
import { A as x } from './a.YKMG9Usu.js'
function Es(m) {
	let n
	return {
		c() {
			n = u('videos, articles, and more media')
		},
		l(t) {
			n = h(t, 'videos, articles, and more media')
		},
		m(t, f) {
			s(t, n, f)
		},
		d(t) {
			t && a(n)
		}
	}
}
function Ms(m) {
	let n
	return {
		c() {
			n = u('believe')
		},
		l(t) {
			n = h(t, 'believe')
		},
		m(t, f) {
			s(t, n, f)
		},
		d(t) {
			t && a(n)
		}
	}
}
function Hs(m) {
	let n
	return {
		c() {
			n = u('A letter calling for pausing AI development')
		},
		l(t) {
			n = h(t, 'A letter calling for pausing AI development')
		},
		m(t, f) {
			s(t, n, f)
		},
		d(t) {
			t && a(n)
		}
	}
}
function Gs(m) {
	let n
	return {
		c() {
			n = u(
				'“If we pursue [our current approach], then we will eventually lose control over the machines”'
			)
		},
		l(t) {
			n = h(
				t,
				'“If we pursue [our current approach], then we will eventually lose control over the machines”'
			)
		},
		m(t, f) {
			s(t, n, f)
		},
		d(t) {
			t && a(n)
		}
	}
}
function Os(m) {
	let n
	return {
		c() {
			n = u(
				'”… rogue AI may be dangerous for the whole of humanity […] banning powerful AI systems (say beyond the abilities of GPT-4) that are given autonomy and agency would be a good start”'
			)
		},
		l(t) {
			n = h(
				t,
				'”… rogue AI may be dangerous for the whole of humanity […] banning powerful AI systems (say beyond the abilities of GPT-4) that are given autonomy and agency would be a good start”'
			)
		},
		m(t, f) {
			s(t, n, f)
		},
		d(t) {
			t && a(n)
		}
	}
}
function Ws(m) {
	let n
	return {
		c() {
			n = u(
				'“The development of full artificial intelligence could spell the end of the human race”'
			)
		},
		l(t) {
			n = h(
				t,
				'“The development of full artificial intelligence could spell the end of the human race”'
			)
		},
		m(t, f) {
			s(t, n, f)
		},
		d(t) {
			t && a(n)
		}
	}
}
function Ss(m) {
	let n
	return {
		c() {
			n = u('left Google')
		},
		l(t) {
			n = h(t, 'left Google')
		},
		m(t, f) {
			s(t, n, f)
		},
		d(t) {
			t && a(n)
		}
	}
}
function qs(m) {
	let n
	return {
		c() {
			n = u('“This is an existential risk”')
		},
		l(t) {
			n = h(t, '“This is an existential risk”')
		},
		m(t, f) {
			s(t, n, f)
		},
		d(t) {
			t && a(n)
		}
	}
}
function Rs(m) {
	let n
	return {
		c() {
			n = u('“If we go ahead on this everyone will die”')
		},
		l(t) {
			n = h(t, '“If we go ahead on this everyone will die”')
		},
		m(t, f) {
			s(t, n, f)
		},
		d(t) {
			t && a(n)
		}
	}
}
function js(m) {
	let n
	return {
		c() {
			n = u(
				'“Development of superhuman machine intelligence is probably the greatest threat to the continued existence of humanity.”'
			)
		},
		l(t) {
			n = h(
				t,
				'“Development of superhuman machine intelligence is probably the greatest threat to the continued existence of humanity.”'
			)
		},
		m(t, f) {
			s(t, n, f)
		},
		d(t) {
			t && a(n)
		}
	}
}
function zs(m) {
	let n
	return {
		c() {
			n = u('“AI has the potential of civilizational destruction”')
		},
		l(t) {
			n = h(t, '“AI has the potential of civilizational destruction”')
		},
		m(t, f) {
			s(t, n, f)
		},
		d(t) {
			t && a(n)
		}
	}
}
function Bs(m) {
	let n
	return {
		c() {
			n = u('“AI could decide that humans are a threat”')
		},
		l(t) {
			n = h(t, '“AI could decide that humans are a threat”')
		},
		m(t, f) {
			s(t, n, f)
		},
		d(t) {
			t && a(n)
		}
	}
}
function Ds(m) {
	let n
	return {
		c() {
			n = u(
				'“I’ve not met anyone in AI labs who says the risk [from training a next-gen model] is less than 1% of blowing up the planet. It’s important that people know lives are being risked.”'
			)
		},
		l(t) {
			n = h(
				t,
				'“I’ve not met anyone in AI labs who says the risk [from training a next-gen model] is less than 1% of blowing up the planet. It’s important that people know lives are being risked.”'
			)
		},
		m(t, f) {
			s(t, n, f)
		},
		d(t) {
			t && a(n)
		}
	}
}
function Ns(m) {
	let n
	return {
		c() {
			n = u('signed the following statement')
		},
		l(t) {
			n = h(t, 'signed the following statement')
		},
		m(t, f) {
			s(t, n, f)
		},
		d(t) {
			t && a(n)
		}
	}
}
function Us(m) {
	let n
	return {
		c() {
			n = u('here')
		},
		l(t) {
			n = h(t, 'here')
		},
		m(t, f) {
			s(t, n, f)
		},
		d(t) {
			t && a(n)
		}
	}
}
function Ys(m) {
	let n
	return {
		c() {
			n = u('here')
		},
		l(t) {
			n = h(t, 'here')
		},
		m(t, f) {
			s(t, n, f)
		},
		d(t) {
			t && a(n)
		}
	}
}
function Ks(m) {
	let n
	return {
		c() {
			n = u('Hack into other computers')
		},
		l(t) {
			n = h(t, 'Hack into other computers')
		},
		m(t, f) {
			s(t, n, f)
		},
		d(t) {
			t && a(n)
		}
	}
}
function Fs(m) {
	let n
	return {
		c() {
			n = u('protein folding')
		},
		l(t) {
			n = h(t, 'protein folding')
		},
		m(t, f) {
			s(t, n, f)
		},
		d(t) {
			t && a(n)
		}
	}
}
function Js(m) {
	let n
	return {
		c() {
			n = u('many examples')
		},
		l(t) {
			n = h(t, 'many examples')
		},
		m(t, f) {
			s(t, n, f)
		},
		d(t) {
			t && a(n)
		}
	}
}
function Xs(m) {
	let n
	return {
		c() {
			n = u('instrumental convergence')
		},
		l(t) {
			n = h(t, 'instrumental convergence')
		},
		m(t, f) {
			s(t, n, f)
		},
		d(t) {
			t && a(n)
		}
	}
}
function Qs(m) {
	let n
	return {
		c() {
			n = u('AutoGPT')
		},
		l(t) {
			n = h(t, 'AutoGPT')
		},
		m(t, f) {
			s(t, n, f)
		},
		d(t) {
			t && a(n)
		}
	}
}
function Zs(m) {
	let n
	return {
		c() {
			n = u('ChaosGPT')
		},
		l(t) {
			n = h(t, 'ChaosGPT')
		},
		m(t, f) {
			s(t, n, f)
		},
		d(t) {
			t && a(n)
		}
	}
}
function Vs(m) {
	let n
	return {
		c() {
			n = u('Tsar Bomba')
		},
		l(t) {
			n = h(t, 'Tsar Bomba')
		},
		m(t, f) {
			s(t, n, f)
		},
		d(t) {
			t && a(n)
		}
	}
}
function el(m) {
	let n
	return {
		c() {
			n = u('proposing')
		},
		l(t) {
			n = h(t, 'proposing')
		},
		m(t, f) {
			s(t, n, f)
		},
		d(t) {
			t && a(n)
		}
	}
}
function tl(m) {
	let n
	return {
		c() {
			n = u('real examples')
		},
		l(t) {
			n = h(t, 'real examples')
		},
		m(t, f) {
			s(t, n, f)
		},
		d(t) {
			t && a(n)
		}
	}
}
function nl(m) {
	let n
	return {
		c() {
			n = u('the average prediction')
		},
		l(t) {
			n = h(t, 'the average prediction')
		},
		m(t, f) {
			s(t, n, f)
		},
		d(t) {
			t && a(n)
		}
	}
}
function ol(m) {
	let n
	return {
		c() {
			n = u('Read more about urgency')
		},
		l(t) {
			n = h(t, 'Read more about urgency')
		},
		m(t, f) {
			s(t, n, f)
		},
		d(t) {
			t && a(n)
		}
	}
}
function al(m) {
	let n
	return {
		c() {
			n = u('psychology of x-risk')
		},
		l(t) {
			n = h(t, 'psychology of x-risk')
		},
		m(t, f) {
			s(t, n, f)
		},
		d(t) {
			t && a(n)
		}
	}
}
function sl(m) {
	let n
	return {
		c() {
			n = u('plan')
		},
		l(t) {
			n = h(t, 'plan')
		},
		m(t, f) {
			s(t, n, f)
		},
		d(t) {
			t && a(n)
		}
	}
}
function ll(m) {
	let n
	return {
		c() {
			n = u('openly admits')
		},
		l(t) {
			n = h(t, 'openly admits')
		},
		m(t, f) {
			s(t, n, f)
		},
		d(t) {
			t && a(n)
		}
	}
}
function il(m) {
	let n
	return {
		c() {
			n = u('This is why we need an international treaty to PauseAI.')
		},
		l(t) {
			n = h(t, 'This is why we need an international treaty to PauseAI.')
		},
		m(t, f) {
			s(t, n, f)
		},
		d(t) {
			t && a(n)
		}
	}
}
function rl(m) {
	let n,
		t,
		f,
		Qe,
		I,
		A,
		hn = 'Experts are sounding the alarm',
		pn,
		O,
		po,
		Q,
		co,
		qt,
		Ia = 'superintelligent',
		go,
		cn,
		Ze,
		Aa =
			'Would you choose to be a passenger on a test flight of a new plane when airplane engineers think there’s a 14% chance that it will crash?',
		dn,
		Ve,
		Z,
		$o,
		gn,
		et,
		Ca = 'The list includes people like:',
		$n,
		V,
		tt,
		Rt,
		ka = 'Stuart Russell',
		wo,
		ee,
		vo,
		nt,
		jt,
		Ta = 'Yoshua Bengio',
		_o,
		te,
		wn,
		ot,
		Pa =
			'But this is not the only time that we’ve been warned about the existential dangers of AI:',
		vn,
		W,
		ne,
		zt,
		La = 'Stephen Hawking',
		bo,
		oe,
		yo,
		xo,
		z,
		Bt,
		Ea = 'Geoffrey Hinton',
		Io,
		ae,
		Ao,
		se,
		Co,
		le,
		Dt,
		Ma = 'Eliezer Yudkowsky',
		ko,
		ie,
		To,
		_n,
		at,
		Ha = 'Even the leaders and investors of the AI companies themselves are warning us:',
		bn,
		T,
		re,
		Nt,
		Ga = 'Sam Altman',
		Po,
		ue,
		Lo,
		Eo,
		st,
		Ut,
		Oa = 'Elon Musk',
		Mo,
		he,
		Ho,
		fe,
		Yt,
		Wa = 'Bill Gates',
		Go,
		me,
		Oo,
		Wo,
		lt,
		Kt,
		Sa = 'Jaan Tallinn',
		So,
		pe,
		yn,
		ce,
		qo,
		de,
		Ro,
		xn,
		it,
		qa =
			'<p>“Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.”</p>',
		In,
		Ft,
		B,
		jo,
		ge,
		zo,
		$e,
		Bo,
		An,
		we,
		Ra = 'What a superintelligent AI can (be used to) do',
		Cn,
		rt,
		ja = `You might think that a superintelligent AI would be locked inside a computer, and therefore can’t affect the real world.
However, we tend to give AI systems access to the internet, which means that they can do a lot of things:`,
		kn,
		C,
		Jt,
		ve,
		Do,
		No,
		Xt,
		za =
			'Manipulate people through fake messages, e-mails, bank transfers, videos or phone calls. Humans could become the AI’s limbs, without even knowing it.',
		Uo,
		Qt,
		Ba =
			'Directly control devices connected to the internet, like cars, planes, robotized (autonomous) weapons or even nuclear weapons.',
		Yo,
		ut,
		Ko,
		_e,
		Fo,
		Jo,
		Zt,
		Da =
			'Trigger a nuclear war by convincing humans that another country is (about to) launch a nuclear attack.',
		Tn,
		be,
		Na = 'The alignment problem: why an AI might lead to human extinction',
		Pn,
		ht,
		Ua = `The type of intelligence we are concerned about can be defined as <em>how good something is at achieving its goals</em>.
Right now, humans are the most intelligent thing on earth, although that could change soon.
Because of our intelligence, we are dominating our planet.
We might not have claws or scaled skin, but we have big brains.
Intelligence is our weapon: it’s what gave us spears, guns and pesticides.
Our intelligence helped us to transform most of the earth into how we like it: cities, buildings, and roads.`,
		Ln,
		ft,
		Ya = `From the perspective of less intelligent animals, this has been a disaster.
It’s not that humans hate the animals, it’s just that we can use their habitats for our own goals.
Our goals are shaped by evolution and include things like comfort, status, love and tasty food.
We are destroying the habitats of other animals as a <strong>side effect of pursuing our goals</strong>.`,
		En,
		P,
		Xo,
		Vt,
		Ka = 'we don’t know how to get them to want what we want',
		Qo,
		en,
		Fa = 'alignment problem',
		Zo,
		ye,
		Vo,
		Mn,
		mt,
		Ja =
			'The examples from the video linked above can be funny or cute, but if a superintelligent system is built, and it has a goal that is even <em>a little</em> different from what we want it to have, it could have disastrous consequences.',
		Hn,
		xe,
		Xa = 'Why most goals are bad news for humans',
		Gn,
		pt,
		Qa = `An AI could have any goal, depending on how it’s trained and prompted (used).
Maybe it wants to calculate pi, maybe it wants to cure cancer, maybe it wants to self-improve.
But even though we cannot tell what a superintelligence will want to achieve, we can make predictions about its sub-goals.`,
		On,
		ct,
		Za =
			'<li><strong>Maximizing its resources</strong>. Harnessing more computers will help an AI achieve its goals. At first, it can achieve this by hacking other computers. Later it may decide that it is more efficient to build its own.</li> <li><strong>Ensuring its own survival</strong>. The AI will not want to be turned off, as it could no longer achieve its goals. AI might conclude that humans are a threat to its existence, as humans could turn it off.</li> <li><strong>Preserving its goals</strong>. The AI will not want humans to modify its code, because that could change its goals, thus preventing it from achieving its current goal.</li>',
		Wn,
		Ie,
		ea,
		Ae,
		ta,
		Sn,
		Ce,
		Va = 'Even a chatbot might be dangerous if it is smart enough',
		qn,
		dt,
		es = `You might wonder: how can a statistical model that predicts the next word in a chat interface pose any danger?
You might say: It’s not conscious, it’s just a bunch of numbers and code.
And yes, we don’t think LLMs are conscious, but that doesn’t mean they can’t be dangerous.`,
		Rn,
		S,
		na,
		ke,
		oa,
		tn,
		ts = 'autonomous agent',
		aa,
		jn,
		L,
		sa,
		Te,
		la,
		Pe,
		ia,
		nn,
		ns = 'it wasn’t that smart',
		ra,
		zn,
		gt,
		os = `Capabilities keep improving due to innovations in training, algorithms, prompting and hardware.
As such, the threat from language models will continue to increase.`,
		Bn,
		Le,
		as = 'Evolution selects for things that are good at surviving',
		Dn,
		$t,
		ss = `AI models, like all living things, are prone to evolutionary pressures, but
there are a few key differences between the evolution of AI models and living things like animals:`,
		Nn,
		wt,
		ls =
			'<li>AI models do not <em>replicate themselves</em>. We replicate them by making copies of their code, or by replicating training software that leads to good models. Code that is useful is copied more often and is used for inspiration to build new models.</li> <li>AI models do not <em>mutate</em> like living things do, but we do make iterations of them where we change how they work. This process is way more deliberate and fast. AI researchers are designing new algorithms, datasets and hardware to make AI models more capable.</li> <li>The <em>environment does not select</em> for fitter AI models, but we do. We select AI models that are useful to us, and we discard the ones that are not. This process does lead to ever more capable and autonomous AI models.</li>',
		Un,
		vt,
		is = `So this system leads to ever more powerful, capable and autonomous AI models - but not necessarily to something that wants to take over, right?
Well, not exactly.
This is because evolution is <em>always</em> selecting for things that are <em>self-preserving</em>.
If we keep trying variations of AI models and different prompts, at some point one instance will try to preserve itself.
We have already discussed why this is likely to happen early on: because self-preservation is always useful to achieve goals.
But even if this is not very likely to happen, it is prone to happen eventually, simply because we keep trying new things with different AI models.`,
		Yn,
		_t,
		rs = `The instance that tries to self-preserve is the one that takes over.
Even if we assume that almost every AI model will behave just fine, <em>a single rogue AI is all it takes</em>.`,
		Kn,
		Ee,
		us = 'After solving the alignment problem: the concentration of Power',
		Fn,
		bt,
		hs = `We haven’t solved the alignment problem yet, but let’s imagine what might happen if we did.
Imagine that a superintelligent AI is built, and it does exactly what the operator wants it to do (not what it <em>asks</em>, but what it <em>wants</em>).
Some person or company would end up controlling this AI and could use this to their advantage.`,
		Jn,
		q,
		ua,
		on,
		fs = 'unimaginable',
		ha,
		Me,
		fa,
		Xn,
		He,
		ms = 'Silicon vs Carbon',
		Qn,
		yt,
		ps = 'We should consider the advantages that a smart piece of software may have over us:',
		Zn,
		xt,
		cs =
			'<li><strong>Speed</strong>: Computers operate at extremely high speeds compared to brains. Human neurons fire about 100 times a second, whereas silicon transistors can switch a billion times a second.</li> <li><strong>Location</strong>: An AI is not constrained to one body - it can be in many locations at once. We have built the infrastructure for it: the internet.</li> <li><strong>Physical limits</strong>: We cannot add more brains to our skulls and become smarter. An AI could dramatically improve its capabilities by adding hardware, like more memory, more processing power, and more sensors (cameras, microphones). An AI could also extend its ‘body’ by controlling connected devices.</li> <li><strong>Materials</strong>: Humans are made of organic materials. Our bodies no longer work if they are too warm or cold, they need food, they need oxygen. Machines can be built from more robust materials, like metals, and can operate in a much wider range of environments.</li> <li><strong>Collaboration</strong>: Humans can collaborate, but it is difficult and time-consuming, so we often fail to coordinate well. An AI could collaborate complex information with replicas of itself at high speed because it can communicate at the speed that data can be sent over the internet.</li>',
		Vn,
		It,
		ds = 'A superintelligent AI will have many advantages to outcompete us.',
		eo,
		Ge,
		gs = 'Why can’t we just turn it off if it’s dangerous?',
		to,
		R,
		ma,
		an,
		$s = 'those that are much smarter than us',
		pa,
		Oe,
		ca,
		no,
		We,
		ws = 'We may not have much time left',
		oo,
		Se,
		da,
		qe,
		ga,
		ao,
		At,
		vs = `It’s hard to predict how long it will take to build a superintelligent AI, but we know that there are more people than ever working on it and that the field is moving at a frantic pace.
It may take many years or just a few months, but we should err on the side of caution, and act now.`,
		so,
		Ct,
		Re,
		$a,
		lo,
		je,
		_s = 'We are not taking the risk seriously enough',
		io,
		kt,
		bs = `The human mind is prone to under-respond to risks that are invisible, slow-moving, and hard to understand.
We also tend to underestimate exponential growth, and we are prone to denial when we are faced with threats to our existence.`,
		ro,
		ze,
		wa,
		Be,
		va,
		uo,
		De,
		ys = 'AI companies are locked in a race to the bottom',
		ho,
		j,
		_a,
		Ne,
		ba,
		Ue,
		ya,
		fo,
		sn,
		Ye,
		mo
	return (
		(f = new x({ props: { href: '/learn', $$slots: { default: [Es] }, $$scope: { ctx: m } } })),
		(Q = new x({
			props: {
				href: 'https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/',
				rel: 'nofollow',
				$$slots: { default: [Ms] },
				$$scope: { ctx: m }
			}
		})),
		(Z = new x({
			props: {
				href: 'https://futureoflife.org/open-letter/pause-giant-ai-experiments/',
				rel: 'nofollow',
				$$slots: { default: [Hs] },
				$$scope: { ctx: m }
			}
		})),
		(ee = new x({
			props: {
				href: 'https://news.berkeley.edu/2023/04/07/stuart-russell-calls-for-new-approach-for-ai-a-civilization-ending-technology/',
				rel: 'nofollow',
				$$slots: { default: [Gs] },
				$$scope: { ctx: m }
			}
		})),
		(te = new x({
			props: {
				href: 'https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/',
				rel: 'nofollow',
				$$slots: { default: [Os] },
				$$scope: { ctx: m }
			}
		})),
		(oe = new x({
			props: {
				href: 'https://nypost.com/2023/05/01/stephen-hawking-warned-ai-could-mean-the-end-of-the-human-race/',
				rel: 'nofollow',
				$$slots: { default: [Ws] },
				$$scope: { ctx: m }
			}
		})),
		(ae = new x({
			props: {
				href: 'https://fortune.com/2023/05/01/godfather-ai-geoffrey-hinton-quit-google-regrets-lifes-work-bad-actors/',
				rel: 'nofollow',
				$$slots: { default: [Ss] },
				$$scope: { ctx: m }
			}
		})),
		(se = new x({
			props: {
				href: 'https://www.reuters.com/technology/ai-pioneer-says-its-threat-world-may-be-more-urgent-than-climate-change-2023-05-05/',
				rel: 'nofollow',
				$$slots: { default: [qs] },
				$$scope: { ctx: m }
			}
		})),
		(ie = new x({
			props: {
				href: 'https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/',
				rel: 'nofollow',
				$$slots: { default: [Rs] },
				$$scope: { ctx: m }
			}
		})),
		(ue = new x({
			props: {
				href: 'https://blog.samaltman.com/machine-intelligence-part-1',
				rel: 'nofollow',
				$$slots: { default: [js] },
				$$scope: { ctx: m }
			}
		})),
		(he = new x({
			props: {
				href: 'https://www.inc.com/ben-sherry/elon-musk-ai-has-the-potential-of-civilizational-destruction.html',
				rel: 'nofollow',
				$$slots: { default: [zs] },
				$$scope: { ctx: m }
			}
		})),
		(me = new x({
			props: {
				href: 'https://www.denisonforum.org/daily-article/bill-gates-ai-humans-threat/',
				rel: 'nofollow',
				$$slots: { default: [Bs] },
				$$scope: { ctx: m }
			}
		})),
		(pe = new x({
			props: {
				href: 'https://twitter.com/liron/status/1656929936639430657',
				rel: 'nofollow',
				$$slots: { default: [Ds] },
				$$scope: { ctx: m }
			}
		})),
		(de = new x({
			props: {
				href: 'https://www.safe.ai/statement-on-ai-risk',
				rel: 'nofollow',
				$$slots: { default: [Ns] },
				$$scope: { ctx: m }
			}
		})),
		(ge = new x({ props: { href: '/quotes', $$slots: { default: [Us] }, $$scope: { ctx: m } } })),
		($e = new x({
			props: { href: '/polls-and-surveys', $$slots: { default: [Ys] }, $$scope: { ctx: m } }
		})),
		(ve = new x({
			props: { href: '/cybersecurity-risks', $$slots: { default: [Ks] }, $$scope: { ctx: m } }
		})),
		(_e = new x({
			props: {
				href: 'https://alphafold.ebi.ac.uk',
				rel: 'nofollow',
				$$slots: { default: [Fs] },
				$$scope: { ctx: m }
			}
		})),
		(ye = new x({
			props: {
				href: 'https://www.youtube.com/watch?v=nKJlF-olKmg',
				rel: 'nofollow',
				$$slots: { default: [Js] },
				$$scope: { ctx: m }
			}
		})),
		(Ae = new x({
			props: {
				href: 'https://www.youtube.com/watch?v=ZeecOKBus3Q',
				rel: 'nofollow',
				$$slots: { default: [Xs] },
				$$scope: { ctx: m }
			}
		})),
		(ke = new x({
			props: {
				href: 'https://github.com/Significant-Gravitas/Auto-GPT',
				rel: 'nofollow',
				$$slots: { default: [Qs] },
				$$scope: { ctx: m }
			}
		})),
		(Te = new x({
			props: {
				href: 'https://www.youtube.com/watch?v=g7YJIpkk7KM',
				rel: 'nofollow',
				$$slots: { default: [Zs] },
				$$scope: { ctx: m }
			}
		})),
		(Pe = new x({
			props: {
				href: 'https://en.wikipedia.org/wiki/Tsar_Bomba',
				rel: 'nofollow',
				$$slots: { default: [Vs] },
				$$scope: { ctx: m }
			}
		})),
		(Me = new x({ props: { href: '/proposal', $$slots: { default: [el] }, $$scope: { ctx: m } } })),
		(Oe = new x({
			props: {
				href: 'https://www.pcmag.com/news/gpt-4-was-able-to-hire-and-deceive-a-human-worker-into-completing-a-task',
				rel: 'nofollow',
				$$slots: { default: [tl] },
				$$scope: { ctx: m }
			}
		})),
		(qe = new x({
			props: {
				href: 'https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/',
				rel: 'nofollow',
				$$slots: { default: [nl] },
				$$scope: { ctx: m }
			}
		})),
		(Re = new x({ props: { href: '/urgency', $$slots: { default: [ol] }, $$scope: { ctx: m } } })),
		(Be = new x({
			props: { href: '/psychology-of-x-risk', $$slots: { default: [al] }, $$scope: { ctx: m } }
		})),
		(Ne = new x({
			props: {
				href: 'https://openai.com/blog/introducing-superalignment',
				rel: 'nofollow',
				$$slots: { default: [sl] },
				$$scope: { ctx: m }
			}
		})),
		(Ue = new x({
			props: {
				href: 'https://www.anthropic.com/index/core-views-on-ai-safety',
				rel: 'nofollow',
				$$slots: { default: [ll] },
				$$scope: { ctx: m }
			}
		})),
		(Ye = new x({ props: { href: '/proposal', $$slots: { default: [il] }, $$scope: { ctx: m } } })),
		{
			c() {
				;(n = i('p')),
					(t = u('You can learn about x-risks reading this page, or you can also learn through ')),
					$(f.$$.fragment),
					(Qe = u('.')),
					(I = p()),
					(A = i('h2')),
					(A.textContent = hn),
					(pn = p()),
					(O = i('p')),
					(po = u('AI researchers on average ')),
					$(Q.$$.fragment),
					(co = u(' there’s a 14% chance that once we build a ')),
					(qt = i('em')),
					(qt.textContent = Ia),
					(go = u(
						' AI (an AI vastly more intelligent than humans), it will lead to “very bad outcomes (e.g. human extinction)“.'
					)),
					(cn = p()),
					(Ze = i('p')),
					(Ze.textContent = Aa),
					(dn = p()),
					(Ve = i('p')),
					$(Z.$$.fragment),
					($o = u(
						' launched in April 2023, and has been signed over 33,000 times, including by many AI researchers and tech leaders.'
					)),
					(gn = p()),
					(et = i('p')),
					(et.textContent = Ca),
					($n = p()),
					(V = i('ul')),
					(tt = i('li')),
					(Rt = i('strong')),
					(Rt.textContent = ka),
					(wo = u(
						', writer of the #1 textbook on Artificial Intelligence used in most AI studies: '
					)),
					$(ee.$$.fragment),
					(vo = p()),
					(nt = i('li')),
					(jt = i('strong')),
					(jt.textContent = Ta),
					(_o = u(', deep learning pioneer and winner of the Turing Award: ')),
					$(te.$$.fragment),
					(wn = p()),
					(ot = i('p')),
					(ot.textContent = Pa),
					(vn = p()),
					(W = i('ul')),
					(ne = i('li')),
					(zt = i('strong')),
					(zt.textContent = La),
					(bo = u(', theoretical physicist & cosmologist: ')),
					$(oe.$$.fragment),
					(yo = u('.')),
					(xo = p()),
					(z = i('li')),
					(Bt = i('strong')),
					(Bt.textContent = Ea),
					(Io = u(', the “Godfather of AI” and Turing Award winner, ')),
					$(ae.$$.fragment),
					(Ao = u(' to warn people of AI: ')),
					$(se.$$.fragment),
					(Co = p()),
					(le = i('li')),
					(Dt = i('strong')),
					(Dt.textContent = Ma),
					(ko = u(', founder of MIRI and conceptual father of the AI safety field: ')),
					$(ie.$$.fragment),
					(To = u('.')),
					(_n = p()),
					(at = i('p')),
					(at.textContent = Ha),
					(bn = p()),
					(T = i('ul')),
					(re = i('li')),
					(Nt = i('strong')),
					(Nt.textContent = Ga),
					(Po = u(' (yes, the CEO of OpenAI who builds ChatGPT): ')),
					$(ue.$$.fragment),
					(Lo = u('.')),
					(Eo = p()),
					(st = i('li')),
					(Ut = i('strong')),
					(Ut.textContent = Oa),
					(Mo = u(', co-founder of OpenAI, SpaceX and Tesla: ')),
					$(he.$$.fragment),
					(Ho = p()),
					(fe = i('li')),
					(Yt = i('strong')),
					(Yt.textContent = Wa),
					(Go = u(' (co-founder of Microsoft, which owns 50% of OpenAI) warned that ')),
					$(me.$$.fragment),
					(Oo = u('.')),
					(Wo = p()),
					(lt = i('li')),
					(Kt = i('strong')),
					(Kt.textContent = Sa),
					(So = u(' (lead investor of Anthropic): ')),
					$(pe.$$.fragment),
					(yn = p()),
					(ce = i('p')),
					(qo = u('The leaders of the 3 top AI labs and hundreds of AI scientists have ')),
					$(de.$$.fragment),
					(Ro = u(' in May 2023:')),
					(xn = p()),
					(it = i('blockquote')),
					(it.innerHTML = qa),
					(In = p()),
					(Ft = i('p')),
					(B = i('strong')),
					(jo = u(
						'You can read a much longer list of similar statements from politicians, CEOs and experts '
					)),
					$(ge.$$.fragment),
					(zo = u(' and other similar polls on the experts (and the public) ')),
					$($e.$$.fragment),
					(Bo = u('.')),
					(An = p()),
					(we = i('h2')),
					(we.textContent = Ra),
					(Cn = p()),
					(rt = i('p')),
					(rt.textContent = ja),
					(kn = p()),
					(C = i('ul')),
					(Jt = i('li')),
					$(ve.$$.fragment),
					(Do = u(
						', including all smartphones, laptops, server farms, etc. It could use the sensors of these devices as its eyes and ears, having digital senses everywhere.'
					)),
					(No = p()),
					(Xt = i('li')),
					(Xt.textContent = za),
					(Uo = p()),
					(Qt = i('li')),
					(Qt.textContent = Ba),
					(Yo = p()),
					(ut = i('li')),
					(Ko = u('Design a novel bioweapon, e.g. by combining viral strands or by using ')),
					$(_e.$$.fragment),
					(Fo = u(' and order it to be printed in a lab.')),
					(Jo = p()),
					(Zt = i('li')),
					(Zt.textContent = Da),
					(Tn = p()),
					(be = i('h2')),
					(be.textContent = Na),
					(Pn = p()),
					(ht = i('p')),
					(ht.innerHTML = Ua),
					(Ln = p()),
					(ft = i('p')),
					(ft.innerHTML = Ya),
					(En = p()),
					(P = i('p')),
					(Xo = u(`An AI can also have goals.
We know how to train machines to be intelligent, but `)),
					(Vt = i('strong')),
					(Vt.textContent = Ka),
					(Qo = u(`.
We don’t even know what goals the machines will pursue after we train them.
The problem of getting an AI to want what we want is called the `)),
					(en = i('em')),
					(en.textContent = Fa),
					(Zo = u(`.
This is not a hypothetical problem - there are `)),
					$(ye.$$.fragment),
					(Vo = u(' of AI systems learning to want the wrong thing.')),
					(Mn = p()),
					(mt = i('p')),
					(mt.innerHTML = Ja),
					(Hn = p()),
					(xe = i('h2')),
					(xe.textContent = Xa),
					(Gn = p()),
					(pt = i('p')),
					(pt.textContent = Qa),
					(On = p()),
					(ct = i('ul')),
					(ct.innerHTML = Za),
					(Wn = p()),
					(Ie = i('p')),
					(ea = u('The tendency to pursue these subgoals given any high-level goal is called ')),
					$(Ae.$$.fragment),
					(ta = u(', and it is a key concern for AI safety researchers.')),
					(Sn = p()),
					(Ce = i('h2')),
					(Ce.textContent = Va),
					(qn = p()),
					(dt = i('p')),
					(dt.textContent = es),
					(Rn = p()),
					(S = i('p')),
					(na = u(`LLMs, like GPT, are trained to predict or mimic virtually any line of thought.
It could mimic a helpful mentor, but also someone with bad intentions, a ruthless dictator or a psychopath.
With the usage of tools like `)),
					$(ke.$$.fragment),
					(oa = u(', a chatbot could be turned into an ')),
					(tn = i('em')),
					(tn.textContent = ts),
					(aa = u(': an AI that pursues any goal it is given, without any human intervention.')),
					(jn = p()),
					(L = i('p')),
					(sa = u('Take ')),
					$(Te.$$.fragment),
					(la = u(`, for example.
This is an AI, using the aforementioned AutoGPT + GPT-4, that is instructed to “Destroy humanity”.
When it was turned on, it autonomously searched the internet for the most destructive weapon and found the `)),
					$(Pe.$$.fragment),
					(ia = u(`, a 50-megaton nuclear bomb.
It then posted a tweet about it.
Seeing an AI reason about how it will end humanity is both a little funny and terrifying.
Luckily ChaosGPT didn’t get very far in its quest for dominance.
The reason it didn’t get very far: `)),
					(nn = i('em')),
					(nn.textContent = ns),
					(ra = u('.')),
					(zn = p()),
					(gt = i('p')),
					(gt.textContent = os),
					(Bn = p()),
					(Le = i('h2')),
					(Le.textContent = as),
					(Dn = p()),
					($t = i('p')),
					($t.textContent = ss),
					(Nn = p()),
					(wt = i('ul')),
					(wt.innerHTML = ls),
					(Un = p()),
					(vt = i('p')),
					(vt.innerHTML = is),
					(Yn = p()),
					(_t = i('p')),
					(_t.innerHTML = rs),
					(Kn = p()),
					(Ee = i('h2')),
					(Ee.textContent = us),
					(Fn = p()),
					(bt = i('p')),
					(bt.innerHTML = hs),
					(Jn = p()),
					(q = i('p')),
					(ua =
						u(`A superintelligence could be used to create radically new weapons, hack all computers, overthrow governments and manipulate humanity.
The operator would have `)),
					(on = i('em')),
					(on.textContent = fs),
					(ha = u(` power.
Should we trust a single entity with that much power?
We might end up in a utopian world where all diseases are cured and everybody is happy, or in an Orwellian nightmare.
This is why we’re not just `)),
					$(Me.$$.fragment),
					(fa = u(
						' superhuman AI to be provably safe but also to be controlled by a democratic process.'
					)),
					(Xn = p()),
					(He = i('h2')),
					(He.textContent = ms),
					(Qn = p()),
					(yt = i('p')),
					(yt.textContent = ps),
					(Zn = p()),
					(xt = i('ul')),
					(xt.innerHTML = cs),
					(Vn = p()),
					(It = i('p')),
					(It.textContent = ds),
					(eo = p()),
					(Ge = i('h2')),
					(Ge.textContent = gs),
					(to = p()),
					(R = i('p')),
					(ma = u(`For AIs that are not superintelligent, we could.
The core problem is `)),
					(an = i('em')),
					(an.textContent = $s),
					(pa = u(`.
A superintelligence will understand the world around it and will be able to predict how humans respond, especially the ones that are trained on all written human knowledge.
If the AI knows you can turn it off, it might behave nicely until it is certain that it can get rid of you.
We already have `)),
					$(Oe.$$.fragment),
					(ca = u(` of AI systems deceiving humans to achieve their goals.
A superintelligent AI would be a master of deception.`)),
					(no = p()),
					(We = i('h2')),
					(We.textContent = ws),
					(oo = p()),
					(Se = i('p')),
					(da = u('In 2020, ')),
					$(qe.$$.fragment),
					(ga = u(` for weak AGI was 2055.
It now sits at 2026.
The latest LLM revolution has surprised most AI researchers, and the field is moving at a frantic pace.`)),
					(ao = p()),
					(At = i('p')),
					(At.textContent = vs),
					(so = p()),
					(Ct = i('p')),
					$(Re.$$.fragment),
					($a = u('.')),
					(lo = p()),
					(je = i('h2')),
					(je.textContent = _s),
					(io = p()),
					(kt = i('p')),
					(kt.textContent = bs),
					(ro = p()),
					(ze = i('p')),
					(wa = u('Read more about the ')),
					$(Be.$$.fragment),
					(va = u('.')),
					(uo = p()),
					(De = i('h2')),
					(De.textContent = ys),
					(ho = p()),
					(j = i('p')),
					(_a = u(`OpenAI, DeepMind and Anthropic want to develop AI safely.
Unfortunately, they do not know how to do this, and they are forced by various incentives to keep racing faster to get to AGI first.
OpenAI’s `)),
					$(Ne.$$.fragment),
					(ba =
						u(` is to use future AI systems to align AI. The problem with this is that we have no guarantee that we will create an AI that solves alignment before we have an AI that is catastrophically dangerous.
Anthropic `)),
					$(Ue.$$.fragment),
					(ya = u(` that it has no idea yet how to solve the alignment problem.
DeepMind has not publicly stated any plan to solve the Alignment problem.`)),
					(fo = p()),
					(sn = i('p')),
					$(Ye.$$.fragment),
					this.h()
			},
			l(e) {
				n = r(e, 'P', {})
				var o = g(n)
				;(t = h(
					o,
					'You can learn about x-risks reading this page, or you can also learn through '
				)),
					w(f.$$.fragment, o),
					(Qe = h(o, '.')),
					o.forEach(a),
					(I = c(e)),
					(A = r(e, 'H2', { id: !0, 'data-svelte-h': !0 })),
					d(A) !== 'svelte-1pzduqi' && (A.textContent = hn),
					(pn = c(e)),
					(O = r(e, 'P', {}))
				var D = g(O)
				;(po = h(D, 'AI researchers on average ')),
					w(Q.$$.fragment, D),
					(co = h(D, ' there’s a 14% chance that once we build a ')),
					(qt = r(D, 'EM', { 'data-svelte-h': !0 })),
					d(qt) !== 'svelte-2rvvcy' && (qt.textContent = Ia),
					(go = h(
						D,
						' AI (an AI vastly more intelligent than humans), it will lead to “very bad outcomes (e.g. human extinction)“.'
					)),
					D.forEach(a),
					(cn = c(e)),
					(Ze = r(e, 'P', { 'data-svelte-h': !0 })),
					d(Ze) !== 'svelte-1tbvxal' && (Ze.textContent = Aa),
					(dn = c(e)),
					(Ve = r(e, 'P', {}))
				var ln = g(Ve)
				w(Z.$$.fragment, ln),
					($o = h(
						ln,
						' launched in April 2023, and has been signed over 33,000 times, including by many AI researchers and tech leaders.'
					)),
					ln.forEach(a),
					(gn = c(e)),
					(et = r(e, 'P', { 'data-svelte-h': !0 })),
					d(et) !== 'svelte-1wlal5m' && (et.textContent = Ca),
					($n = c(e)),
					(V = r(e, 'UL', {}))
				var Tt = g(V)
				tt = r(Tt, 'LI', {})
				var Pt = g(tt)
				;(Rt = r(Pt, 'STRONG', { 'data-svelte-h': !0 })),
					d(Rt) !== 'svelte-dddtip' && (Rt.textContent = ka),
					(wo = h(
						Pt,
						', writer of the #1 textbook on Artificial Intelligence used in most AI studies: '
					)),
					w(ee.$$.fragment, Pt),
					Pt.forEach(a),
					(vo = c(Tt)),
					(nt = r(Tt, 'LI', {}))
				var Lt = g(nt)
				;(jt = r(Lt, 'STRONG', { 'data-svelte-h': !0 })),
					d(jt) !== 'svelte-1lyopnl' && (jt.textContent = Ta),
					(_o = h(Lt, ', deep learning pioneer and winner of the Turing Award: ')),
					w(te.$$.fragment, Lt),
					Lt.forEach(a),
					Tt.forEach(a),
					(wn = c(e)),
					(ot = r(e, 'P', { 'data-svelte-h': !0 })),
					d(ot) !== 'svelte-j0baor' && (ot.textContent = Pa),
					(vn = c(e)),
					(W = r(e, 'UL', {}))
				var N = g(W)
				ne = r(N, 'LI', {})
				var Ke = g(ne)
				;(zt = r(Ke, 'STRONG', { 'data-svelte-h': !0 })),
					d(zt) !== 'svelte-51mjec' && (zt.textContent = La),
					(bo = h(Ke, ', theoretical physicist & cosmologist: ')),
					w(oe.$$.fragment, Ke),
					(yo = h(Ke, '.')),
					Ke.forEach(a),
					(xo = c(N)),
					(z = r(N, 'LI', {}))
				var U = g(z)
				;(Bt = r(U, 'STRONG', { 'data-svelte-h': !0 })),
					d(Bt) !== 'svelte-dar431' && (Bt.textContent = Ea),
					(Io = h(U, ', the “Godfather of AI” and Turing Award winner, ')),
					w(ae.$$.fragment, U),
					(Ao = h(U, ' to warn people of AI: ')),
					w(se.$$.fragment, U),
					U.forEach(a),
					(Co = c(N)),
					(le = r(N, 'LI', {}))
				var Fe = g(le)
				;(Dt = r(Fe, 'STRONG', { 'data-svelte-h': !0 })),
					d(Dt) !== 'svelte-gy6zk0' && (Dt.textContent = Ma),
					(ko = h(Fe, ', founder of MIRI and conceptual father of the AI safety field: ')),
					w(ie.$$.fragment, Fe),
					(To = h(Fe, '.')),
					Fe.forEach(a),
					N.forEach(a),
					(_n = c(e)),
					(at = r(e, 'P', { 'data-svelte-h': !0 })),
					d(at) !== 'svelte-q3g6xu' && (at.textContent = Ha),
					(bn = c(e)),
					(T = r(e, 'UL', {}))
				var M = g(T)
				re = r(M, 'LI', {})
				var Je = g(re)
				;(Nt = r(Je, 'STRONG', { 'data-svelte-h': !0 })),
					d(Nt) !== 'svelte-1x3nm3w' && (Nt.textContent = Ga),
					(Po = h(Je, ' (yes, the CEO of OpenAI who builds ChatGPT): ')),
					w(ue.$$.fragment, Je),
					(Lo = h(Je, '.')),
					Je.forEach(a),
					(Eo = c(M)),
					(st = r(M, 'LI', {}))
				var Et = g(st)
				;(Ut = r(Et, 'STRONG', { 'data-svelte-h': !0 })),
					d(Ut) !== 'svelte-1761z62' && (Ut.textContent = Oa),
					(Mo = h(Et, ', co-founder of OpenAI, SpaceX and Tesla: ')),
					w(he.$$.fragment, Et),
					Et.forEach(a),
					(Ho = c(M)),
					(fe = r(M, 'LI', {}))
				var Xe = g(fe)
				;(Yt = r(Xe, 'STRONG', { 'data-svelte-h': !0 })),
					d(Yt) !== 'svelte-118yt91' && (Yt.textContent = Wa),
					(Go = h(Xe, ' (co-founder of Microsoft, which owns 50% of OpenAI) warned that ')),
					w(me.$$.fragment, Xe),
					(Oo = h(Xe, '.')),
					Xe.forEach(a),
					(Wo = c(M)),
					(lt = r(M, 'LI', {}))
				var Mt = g(lt)
				;(Kt = r(Mt, 'STRONG', { 'data-svelte-h': !0 })),
					d(Kt) !== 'svelte-q18iks' && (Kt.textContent = Sa),
					(So = h(Mt, ' (lead investor of Anthropic): ')),
					w(pe.$$.fragment, Mt),
					Mt.forEach(a),
					M.forEach(a),
					(yn = c(e)),
					(ce = r(e, 'P', {}))
				var Ht = g(ce)
				;(qo = h(Ht, 'The leaders of the 3 top AI labs and hundreds of AI scientists have ')),
					w(de.$$.fragment, Ht),
					(Ro = h(Ht, ' in May 2023:')),
					Ht.forEach(a),
					(xn = c(e)),
					(it = r(e, 'BLOCKQUOTE', { 'data-svelte-h': !0 })),
					d(it) !== 'svelte-1lott82' && (it.innerHTML = qa),
					(In = c(e)),
					(Ft = r(e, 'P', {}))
				var fn = g(Ft)
				B = r(fn, 'STRONG', {})
				var Y = g(B)
				;(jo = h(
					Y,
					'You can read a much longer list of similar statements from politicians, CEOs and experts '
				)),
					w(ge.$$.fragment, Y),
					(zo = h(Y, ' and other similar polls on the experts (and the public) ')),
					w($e.$$.fragment, Y),
					(Bo = h(Y, '.')),
					Y.forEach(a),
					fn.forEach(a),
					(An = c(e)),
					(we = r(e, 'H2', { id: !0, 'data-svelte-h': !0 })),
					d(we) !== 'svelte-1p1i0f3' && (we.textContent = Ra),
					(Cn = c(e)),
					(rt = r(e, 'P', { 'data-svelte-h': !0 })),
					d(rt) !== 'svelte-mwqgt' && (rt.textContent = ja),
					(kn = c(e)),
					(C = r(e, 'UL', {}))
				var k = g(C)
				Jt = r(k, 'LI', {})
				var rn = g(Jt)
				w(ve.$$.fragment, rn),
					(Do = h(
						rn,
						', including all smartphones, laptops, server farms, etc. It could use the sensors of these devices as its eyes and ears, having digital senses everywhere.'
					)),
					rn.forEach(a),
					(No = c(k)),
					(Xt = r(k, 'LI', { 'data-svelte-h': !0 })),
					d(Xt) !== 'svelte-1o3c5zl' && (Xt.textContent = za),
					(Uo = c(k)),
					(Qt = r(k, 'LI', { 'data-svelte-h': !0 })),
					d(Qt) !== 'svelte-5g6cud' && (Qt.textContent = Ba),
					(Yo = c(k)),
					(ut = r(k, 'LI', {}))
				var Gt = g(ut)
				;(Ko = h(Gt, 'Design a novel bioweapon, e.g. by combining viral strands or by using ')),
					w(_e.$$.fragment, Gt),
					(Fo = h(Gt, ' and order it to be printed in a lab.')),
					Gt.forEach(a),
					(Jo = c(k)),
					(Zt = r(k, 'LI', { 'data-svelte-h': !0 })),
					d(Zt) !== 'svelte-teuzw3' && (Zt.textContent = Da),
					k.forEach(a),
					(Tn = c(e)),
					(be = r(e, 'H2', { id: !0, 'data-svelte-h': !0 })),
					d(be) !== 'svelte-1yuo6r6' && (be.textContent = Na),
					(Pn = c(e)),
					(ht = r(e, 'P', { 'data-svelte-h': !0 })),
					d(ht) !== 'svelte-1k3toim' && (ht.innerHTML = Ua),
					(Ln = c(e)),
					(ft = r(e, 'P', { 'data-svelte-h': !0 })),
					d(ft) !== 'svelte-pw8uf7' && (ft.innerHTML = Ya),
					(En = c(e)),
					(P = r(e, 'P', {}))
				var H = g(P)
				;(Xo = h(
					H,
					`An AI can also have goals.
We know how to train machines to be intelligent, but `
				)),
					(Vt = r(H, 'STRONG', { 'data-svelte-h': !0 })),
					d(Vt) !== 'svelte-1woj6kn' && (Vt.textContent = Ka),
					(Qo = h(
						H,
						`.
We don’t even know what goals the machines will pursue after we train them.
The problem of getting an AI to want what we want is called the `
					)),
					(en = r(H, 'EM', { 'data-svelte-h': !0 })),
					d(en) !== 'svelte-ro9578' && (en.textContent = Fa),
					(Zo = h(
						H,
						`.
This is not a hypothetical problem - there are `
					)),
					w(ye.$$.fragment, H),
					(Vo = h(H, ' of AI systems learning to want the wrong thing.')),
					H.forEach(a),
					(Mn = c(e)),
					(mt = r(e, 'P', { 'data-svelte-h': !0 })),
					d(mt) !== 'svelte-nj9fdk' && (mt.innerHTML = Ja),
					(Hn = c(e)),
					(xe = r(e, 'H2', { id: !0, 'data-svelte-h': !0 })),
					d(xe) !== 'svelte-1gwjzut' && (xe.textContent = Xa),
					(Gn = c(e)),
					(pt = r(e, 'P', { 'data-svelte-h': !0 })),
					d(pt) !== 'svelte-lwgbpr' && (pt.textContent = Qa),
					(On = c(e)),
					(ct = r(e, 'UL', { 'data-svelte-h': !0 })),
					d(ct) !== 'svelte-ywuwvk' && (ct.innerHTML = Za),
					(Wn = c(e)),
					(Ie = r(e, 'P', {}))
				var Ot = g(Ie)
				;(ea = h(Ot, 'The tendency to pursue these subgoals given any high-level goal is called ')),
					w(Ae.$$.fragment, Ot),
					(ta = h(Ot, ', and it is a key concern for AI safety researchers.')),
					Ot.forEach(a),
					(Sn = c(e)),
					(Ce = r(e, 'H2', { id: !0, 'data-svelte-h': !0 })),
					d(Ce) !== 'svelte-5sgsro' && (Ce.textContent = Va),
					(qn = c(e)),
					(dt = r(e, 'P', { 'data-svelte-h': !0 })),
					d(dt) !== 'svelte-1r3umf6' && (dt.textContent = es),
					(Rn = c(e)),
					(S = r(e, 'P', {}))
				var K = g(S)
				;(na = h(
					K,
					`LLMs, like GPT, are trained to predict or mimic virtually any line of thought.
It could mimic a helpful mentor, but also someone with bad intentions, a ruthless dictator or a psychopath.
With the usage of tools like `
				)),
					w(ke.$$.fragment, K),
					(oa = h(K, ', a chatbot could be turned into an ')),
					(tn = r(K, 'EM', { 'data-svelte-h': !0 })),
					d(tn) !== 'svelte-1n9o9lr' && (tn.textContent = ts),
					(aa = h(K, ': an AI that pursues any goal it is given, without any human intervention.')),
					K.forEach(a),
					(jn = c(e)),
					(L = r(e, 'P', {}))
				var G = g(L)
				;(sa = h(G, 'Take ')),
					w(Te.$$.fragment, G),
					(la = h(
						G,
						`, for example.
This is an AI, using the aforementioned AutoGPT + GPT-4, that is instructed to “Destroy humanity”.
When it was turned on, it autonomously searched the internet for the most destructive weapon and found the `
					)),
					w(Pe.$$.fragment, G),
					(ia = h(
						G,
						`, a 50-megaton nuclear bomb.
It then posted a tweet about it.
Seeing an AI reason about how it will end humanity is both a little funny and terrifying.
Luckily ChaosGPT didn’t get very far in its quest for dominance.
The reason it didn’t get very far: `
					)),
					(nn = r(G, 'EM', { 'data-svelte-h': !0 })),
					d(nn) !== 'svelte-wb4r4v' && (nn.textContent = ns),
					(ra = h(G, '.')),
					G.forEach(a),
					(zn = c(e)),
					(gt = r(e, 'P', { 'data-svelte-h': !0 })),
					d(gt) !== 'svelte-d3kfvv' && (gt.textContent = os),
					(Bn = c(e)),
					(Le = r(e, 'H2', { id: !0, 'data-svelte-h': !0 })),
					d(Le) !== 'svelte-1c8py56' && (Le.textContent = as),
					(Dn = c(e)),
					($t = r(e, 'P', { 'data-svelte-h': !0 })),
					d($t) !== 'svelte-jby12f' && ($t.textContent = ss),
					(Nn = c(e)),
					(wt = r(e, 'UL', { 'data-svelte-h': !0 })),
					d(wt) !== 'svelte-1d25s3h' && (wt.innerHTML = ls),
					(Un = c(e)),
					(vt = r(e, 'P', { 'data-svelte-h': !0 })),
					d(vt) !== 'svelte-kycnyp' && (vt.innerHTML = is),
					(Yn = c(e)),
					(_t = r(e, 'P', { 'data-svelte-h': !0 })),
					d(_t) !== 'svelte-gqxb37' && (_t.innerHTML = rs),
					(Kn = c(e)),
					(Ee = r(e, 'H2', { id: !0, 'data-svelte-h': !0 })),
					d(Ee) !== 'svelte-5gdc00' && (Ee.textContent = us),
					(Fn = c(e)),
					(bt = r(e, 'P', { 'data-svelte-h': !0 })),
					d(bt) !== 'svelte-1kfsipc' && (bt.innerHTML = hs),
					(Jn = c(e)),
					(q = r(e, 'P', {}))
				var F = g(q)
				;(ua = h(
					F,
					`A superintelligence could be used to create radically new weapons, hack all computers, overthrow governments and manipulate humanity.
The operator would have `
				)),
					(on = r(F, 'EM', { 'data-svelte-h': !0 })),
					d(on) !== 'svelte-1i5s0bs' && (on.textContent = fs),
					(ha = h(
						F,
						` power.
Should we trust a single entity with that much power?
We might end up in a utopian world where all diseases are cured and everybody is happy, or in an Orwellian nightmare.
This is why we’re not just `
					)),
					w(Me.$$.fragment, F),
					(fa = h(
						F,
						' superhuman AI to be provably safe but also to be controlled by a democratic process.'
					)),
					F.forEach(a),
					(Xn = c(e)),
					(He = r(e, 'H2', { id: !0, 'data-svelte-h': !0 })),
					d(He) !== 'svelte-4bi4t6' && (He.textContent = ms),
					(Qn = c(e)),
					(yt = r(e, 'P', { 'data-svelte-h': !0 })),
					d(yt) !== 'svelte-qz9m5p' && (yt.textContent = ps),
					(Zn = c(e)),
					(xt = r(e, 'UL', { 'data-svelte-h': !0 })),
					d(xt) !== 'svelte-kmbrqs' && (xt.innerHTML = cs),
					(Vn = c(e)),
					(It = r(e, 'P', { 'data-svelte-h': !0 })),
					d(It) !== 'svelte-v1u3ss' && (It.textContent = ds),
					(eo = c(e)),
					(Ge = r(e, 'H2', { id: !0, 'data-svelte-h': !0 })),
					d(Ge) !== 'svelte-1gin4my' && (Ge.textContent = gs),
					(to = c(e)),
					(R = r(e, 'P', {}))
				var J = g(R)
				;(ma = h(
					J,
					`For AIs that are not superintelligent, we could.
The core problem is `
				)),
					(an = r(J, 'EM', { 'data-svelte-h': !0 })),
					d(an) !== 'svelte-gnh4u8' && (an.textContent = $s),
					(pa = h(
						J,
						`.
A superintelligence will understand the world around it and will be able to predict how humans respond, especially the ones that are trained on all written human knowledge.
If the AI knows you can turn it off, it might behave nicely until it is certain that it can get rid of you.
We already have `
					)),
					w(Oe.$$.fragment, J),
					(ca = h(
						J,
						` of AI systems deceiving humans to achieve their goals.
A superintelligent AI would be a master of deception.`
					)),
					J.forEach(a),
					(no = c(e)),
					(We = r(e, 'H2', { id: !0, 'data-svelte-h': !0 })),
					d(We) !== 'svelte-18ijnqq' && (We.textContent = ws),
					(oo = c(e)),
					(Se = r(e, 'P', {}))
				var Wt = g(Se)
				;(da = h(Wt, 'In 2020, ')),
					w(qe.$$.fragment, Wt),
					(ga = h(
						Wt,
						` for weak AGI was 2055.
It now sits at 2026.
The latest LLM revolution has surprised most AI researchers, and the field is moving at a frantic pace.`
					)),
					Wt.forEach(a),
					(ao = c(e)),
					(At = r(e, 'P', { 'data-svelte-h': !0 })),
					d(At) !== 'svelte-6cmjgq' && (At.textContent = vs),
					(so = c(e)),
					(Ct = r(e, 'P', {}))
				var un = g(Ct)
				w(Re.$$.fragment, un),
					($a = h(un, '.')),
					un.forEach(a),
					(lo = c(e)),
					(je = r(e, 'H2', { id: !0, 'data-svelte-h': !0 })),
					d(je) !== 'svelte-1wiv0qf' && (je.textContent = _s),
					(io = c(e)),
					(kt = r(e, 'P', { 'data-svelte-h': !0 })),
					d(kt) !== 'svelte-akbu1s' && (kt.textContent = bs),
					(ro = c(e)),
					(ze = r(e, 'P', {}))
				var St = g(ze)
				;(wa = h(St, 'Read more about the ')),
					w(Be.$$.fragment, St),
					(va = h(St, '.')),
					St.forEach(a),
					(uo = c(e)),
					(De = r(e, 'H2', { id: !0, 'data-svelte-h': !0 })),
					d(De) !== 'svelte-1e6decz' && (De.textContent = ys),
					(ho = c(e)),
					(j = r(e, 'P', {}))
				var X = g(j)
				;(_a = h(
					X,
					`OpenAI, DeepMind and Anthropic want to develop AI safely.
Unfortunately, they do not know how to do this, and they are forced by various incentives to keep racing faster to get to AGI first.
OpenAI’s `
				)),
					w(Ne.$$.fragment, X),
					(ba = h(
						X,
						` is to use future AI systems to align AI. The problem with this is that we have no guarantee that we will create an AI that solves alignment before we have an AI that is catastrophically dangerous.
Anthropic `
					)),
					w(Ue.$$.fragment, X),
					(ya = h(
						X,
						` that it has no idea yet how to solve the alignment problem.
DeepMind has not publicly stated any plan to solve the Alignment problem.`
					)),
					X.forEach(a),
					(fo = c(e)),
					(sn = r(e, 'P', {}))
				var mn = g(sn)
				w(Ye.$$.fragment, mn), mn.forEach(a), this.h()
			},
			h() {
				E(A, 'id', 'experts-are-sounding-the-alarm'),
					E(we, 'id', 'what-a-superintelligent-ai-can-be-used-to-do'),
					E(be, 'id', 'the-alignment-problem-why-an-ai-might-lead-to-human-extinction'),
					E(xe, 'id', 'why-most-goals-are-bad-news-for-humans'),
					E(Ce, 'id', 'even-a-chatbot-might-be-dangerous-if-it-is-smart-enough'),
					E(Le, 'id', 'evolution-selects-for-things-that-are-good-at-surviving'),
					E(Ee, 'id', 'after-solving-the-alignment-problem-the-concentration-of-power'),
					E(He, 'id', 'silicon-vs-carbon'),
					E(Ge, 'id', 'why-cant-we-just-turn-it-off-if-its-dangerous'),
					E(We, 'id', 'we-may-not-have-much-time-left'),
					E(je, 'id', 'we-are-not-taking-the-risk-seriously-enough'),
					E(De, 'id', 'ai-companies-are-locked-in-a-race-to-the-bottom')
			},
			m(e, o) {
				s(e, n, o),
					l(n, t),
					v(f, n, null),
					l(n, Qe),
					s(e, I, o),
					s(e, A, o),
					s(e, pn, o),
					s(e, O, o),
					l(O, po),
					v(Q, O, null),
					l(O, co),
					l(O, qt),
					l(O, go),
					s(e, cn, o),
					s(e, Ze, o),
					s(e, dn, o),
					s(e, Ve, o),
					v(Z, Ve, null),
					l(Ve, $o),
					s(e, gn, o),
					s(e, et, o),
					s(e, $n, o),
					s(e, V, o),
					l(V, tt),
					l(tt, Rt),
					l(tt, wo),
					v(ee, tt, null),
					l(V, vo),
					l(V, nt),
					l(nt, jt),
					l(nt, _o),
					v(te, nt, null),
					s(e, wn, o),
					s(e, ot, o),
					s(e, vn, o),
					s(e, W, o),
					l(W, ne),
					l(ne, zt),
					l(ne, bo),
					v(oe, ne, null),
					l(ne, yo),
					l(W, xo),
					l(W, z),
					l(z, Bt),
					l(z, Io),
					v(ae, z, null),
					l(z, Ao),
					v(se, z, null),
					l(W, Co),
					l(W, le),
					l(le, Dt),
					l(le, ko),
					v(ie, le, null),
					l(le, To),
					s(e, _n, o),
					s(e, at, o),
					s(e, bn, o),
					s(e, T, o),
					l(T, re),
					l(re, Nt),
					l(re, Po),
					v(ue, re, null),
					l(re, Lo),
					l(T, Eo),
					l(T, st),
					l(st, Ut),
					l(st, Mo),
					v(he, st, null),
					l(T, Ho),
					l(T, fe),
					l(fe, Yt),
					l(fe, Go),
					v(me, fe, null),
					l(fe, Oo),
					l(T, Wo),
					l(T, lt),
					l(lt, Kt),
					l(lt, So),
					v(pe, lt, null),
					s(e, yn, o),
					s(e, ce, o),
					l(ce, qo),
					v(de, ce, null),
					l(ce, Ro),
					s(e, xn, o),
					s(e, it, o),
					s(e, In, o),
					s(e, Ft, o),
					l(Ft, B),
					l(B, jo),
					v(ge, B, null),
					l(B, zo),
					v($e, B, null),
					l(B, Bo),
					s(e, An, o),
					s(e, we, o),
					s(e, Cn, o),
					s(e, rt, o),
					s(e, kn, o),
					s(e, C, o),
					l(C, Jt),
					v(ve, Jt, null),
					l(Jt, Do),
					l(C, No),
					l(C, Xt),
					l(C, Uo),
					l(C, Qt),
					l(C, Yo),
					l(C, ut),
					l(ut, Ko),
					v(_e, ut, null),
					l(ut, Fo),
					l(C, Jo),
					l(C, Zt),
					s(e, Tn, o),
					s(e, be, o),
					s(e, Pn, o),
					s(e, ht, o),
					s(e, Ln, o),
					s(e, ft, o),
					s(e, En, o),
					s(e, P, o),
					l(P, Xo),
					l(P, Vt),
					l(P, Qo),
					l(P, en),
					l(P, Zo),
					v(ye, P, null),
					l(P, Vo),
					s(e, Mn, o),
					s(e, mt, o),
					s(e, Hn, o),
					s(e, xe, o),
					s(e, Gn, o),
					s(e, pt, o),
					s(e, On, o),
					s(e, ct, o),
					s(e, Wn, o),
					s(e, Ie, o),
					l(Ie, ea),
					v(Ae, Ie, null),
					l(Ie, ta),
					s(e, Sn, o),
					s(e, Ce, o),
					s(e, qn, o),
					s(e, dt, o),
					s(e, Rn, o),
					s(e, S, o),
					l(S, na),
					v(ke, S, null),
					l(S, oa),
					l(S, tn),
					l(S, aa),
					s(e, jn, o),
					s(e, L, o),
					l(L, sa),
					v(Te, L, null),
					l(L, la),
					v(Pe, L, null),
					l(L, ia),
					l(L, nn),
					l(L, ra),
					s(e, zn, o),
					s(e, gt, o),
					s(e, Bn, o),
					s(e, Le, o),
					s(e, Dn, o),
					s(e, $t, o),
					s(e, Nn, o),
					s(e, wt, o),
					s(e, Un, o),
					s(e, vt, o),
					s(e, Yn, o),
					s(e, _t, o),
					s(e, Kn, o),
					s(e, Ee, o),
					s(e, Fn, o),
					s(e, bt, o),
					s(e, Jn, o),
					s(e, q, o),
					l(q, ua),
					l(q, on),
					l(q, ha),
					v(Me, q, null),
					l(q, fa),
					s(e, Xn, o),
					s(e, He, o),
					s(e, Qn, o),
					s(e, yt, o),
					s(e, Zn, o),
					s(e, xt, o),
					s(e, Vn, o),
					s(e, It, o),
					s(e, eo, o),
					s(e, Ge, o),
					s(e, to, o),
					s(e, R, o),
					l(R, ma),
					l(R, an),
					l(R, pa),
					v(Oe, R, null),
					l(R, ca),
					s(e, no, o),
					s(e, We, o),
					s(e, oo, o),
					s(e, Se, o),
					l(Se, da),
					v(qe, Se, null),
					l(Se, ga),
					s(e, ao, o),
					s(e, At, o),
					s(e, so, o),
					s(e, Ct, o),
					v(Re, Ct, null),
					l(Ct, $a),
					s(e, lo, o),
					s(e, je, o),
					s(e, io, o),
					s(e, kt, o),
					s(e, ro, o),
					s(e, ze, o),
					l(ze, wa),
					v(Be, ze, null),
					l(ze, va),
					s(e, uo, o),
					s(e, De, o),
					s(e, ho, o),
					s(e, j, o),
					l(j, _a),
					v(Ne, j, null),
					l(j, ba),
					v(Ue, j, null),
					l(j, ya),
					s(e, fo, o),
					s(e, sn, o),
					v(Ye, sn, null),
					(mo = !0)
			},
			p(e, o) {
				const D = {}
				o & 2 && (D.$$scope = { dirty: o, ctx: e }), f.$set(D)
				const ln = {}
				o & 2 && (ln.$$scope = { dirty: o, ctx: e }), Q.$set(ln)
				const Tt = {}
				o & 2 && (Tt.$$scope = { dirty: o, ctx: e }), Z.$set(Tt)
				const Pt = {}
				o & 2 && (Pt.$$scope = { dirty: o, ctx: e }), ee.$set(Pt)
				const Lt = {}
				o & 2 && (Lt.$$scope = { dirty: o, ctx: e }), te.$set(Lt)
				const N = {}
				o & 2 && (N.$$scope = { dirty: o, ctx: e }), oe.$set(N)
				const Ke = {}
				o & 2 && (Ke.$$scope = { dirty: o, ctx: e }), ae.$set(Ke)
				const U = {}
				o & 2 && (U.$$scope = { dirty: o, ctx: e }), se.$set(U)
				const Fe = {}
				o & 2 && (Fe.$$scope = { dirty: o, ctx: e }), ie.$set(Fe)
				const M = {}
				o & 2 && (M.$$scope = { dirty: o, ctx: e }), ue.$set(M)
				const Je = {}
				o & 2 && (Je.$$scope = { dirty: o, ctx: e }), he.$set(Je)
				const Et = {}
				o & 2 && (Et.$$scope = { dirty: o, ctx: e }), me.$set(Et)
				const Xe = {}
				o & 2 && (Xe.$$scope = { dirty: o, ctx: e }), pe.$set(Xe)
				const Mt = {}
				o & 2 && (Mt.$$scope = { dirty: o, ctx: e }), de.$set(Mt)
				const Ht = {}
				o & 2 && (Ht.$$scope = { dirty: o, ctx: e }), ge.$set(Ht)
				const fn = {}
				o & 2 && (fn.$$scope = { dirty: o, ctx: e }), $e.$set(fn)
				const Y = {}
				o & 2 && (Y.$$scope = { dirty: o, ctx: e }), ve.$set(Y)
				const k = {}
				o & 2 && (k.$$scope = { dirty: o, ctx: e }), _e.$set(k)
				const rn = {}
				o & 2 && (rn.$$scope = { dirty: o, ctx: e }), ye.$set(rn)
				const Gt = {}
				o & 2 && (Gt.$$scope = { dirty: o, ctx: e }), Ae.$set(Gt)
				const H = {}
				o & 2 && (H.$$scope = { dirty: o, ctx: e }), ke.$set(H)
				const Ot = {}
				o & 2 && (Ot.$$scope = { dirty: o, ctx: e }), Te.$set(Ot)
				const K = {}
				o & 2 && (K.$$scope = { dirty: o, ctx: e }), Pe.$set(K)
				const G = {}
				o & 2 && (G.$$scope = { dirty: o, ctx: e }), Me.$set(G)
				const F = {}
				o & 2 && (F.$$scope = { dirty: o, ctx: e }), Oe.$set(F)
				const J = {}
				o & 2 && (J.$$scope = { dirty: o, ctx: e }), qe.$set(J)
				const Wt = {}
				o & 2 && (Wt.$$scope = { dirty: o, ctx: e }), Re.$set(Wt)
				const un = {}
				o & 2 && (un.$$scope = { dirty: o, ctx: e }), Be.$set(un)
				const St = {}
				o & 2 && (St.$$scope = { dirty: o, ctx: e }), Ne.$set(St)
				const X = {}
				o & 2 && (X.$$scope = { dirty: o, ctx: e }), Ue.$set(X)
				const mn = {}
				o & 2 && (mn.$$scope = { dirty: o, ctx: e }), Ye.$set(mn)
			},
			i(e) {
				mo ||
					(_(f.$$.fragment, e),
					_(Q.$$.fragment, e),
					_(Z.$$.fragment, e),
					_(ee.$$.fragment, e),
					_(te.$$.fragment, e),
					_(oe.$$.fragment, e),
					_(ae.$$.fragment, e),
					_(se.$$.fragment, e),
					_(ie.$$.fragment, e),
					_(ue.$$.fragment, e),
					_(he.$$.fragment, e),
					_(me.$$.fragment, e),
					_(pe.$$.fragment, e),
					_(de.$$.fragment, e),
					_(ge.$$.fragment, e),
					_($e.$$.fragment, e),
					_(ve.$$.fragment, e),
					_(_e.$$.fragment, e),
					_(ye.$$.fragment, e),
					_(Ae.$$.fragment, e),
					_(ke.$$.fragment, e),
					_(Te.$$.fragment, e),
					_(Pe.$$.fragment, e),
					_(Me.$$.fragment, e),
					_(Oe.$$.fragment, e),
					_(qe.$$.fragment, e),
					_(Re.$$.fragment, e),
					_(Be.$$.fragment, e),
					_(Ne.$$.fragment, e),
					_(Ue.$$.fragment, e),
					_(Ye.$$.fragment, e),
					(mo = !0))
			},
			o(e) {
				b(f.$$.fragment, e),
					b(Q.$$.fragment, e),
					b(Z.$$.fragment, e),
					b(ee.$$.fragment, e),
					b(te.$$.fragment, e),
					b(oe.$$.fragment, e),
					b(ae.$$.fragment, e),
					b(se.$$.fragment, e),
					b(ie.$$.fragment, e),
					b(ue.$$.fragment, e),
					b(he.$$.fragment, e),
					b(me.$$.fragment, e),
					b(pe.$$.fragment, e),
					b(de.$$.fragment, e),
					b(ge.$$.fragment, e),
					b($e.$$.fragment, e),
					b(ve.$$.fragment, e),
					b(_e.$$.fragment, e),
					b(ye.$$.fragment, e),
					b(Ae.$$.fragment, e),
					b(ke.$$.fragment, e),
					b(Te.$$.fragment, e),
					b(Pe.$$.fragment, e),
					b(Me.$$.fragment, e),
					b(Oe.$$.fragment, e),
					b(qe.$$.fragment, e),
					b(Re.$$.fragment, e),
					b(Be.$$.fragment, e),
					b(Ne.$$.fragment, e),
					b(Ue.$$.fragment, e),
					b(Ye.$$.fragment, e),
					(mo = !1)
			},
			d(e) {
				e &&
					(a(n),
					a(I),
					a(A),
					a(pn),
					a(O),
					a(cn),
					a(Ze),
					a(dn),
					a(Ve),
					a(gn),
					a(et),
					a($n),
					a(V),
					a(wn),
					a(ot),
					a(vn),
					a(W),
					a(_n),
					a(at),
					a(bn),
					a(T),
					a(yn),
					a(ce),
					a(xn),
					a(it),
					a(In),
					a(Ft),
					a(An),
					a(we),
					a(Cn),
					a(rt),
					a(kn),
					a(C),
					a(Tn),
					a(be),
					a(Pn),
					a(ht),
					a(Ln),
					a(ft),
					a(En),
					a(P),
					a(Mn),
					a(mt),
					a(Hn),
					a(xe),
					a(Gn),
					a(pt),
					a(On),
					a(ct),
					a(Wn),
					a(Ie),
					a(Sn),
					a(Ce),
					a(qn),
					a(dt),
					a(Rn),
					a(S),
					a(jn),
					a(L),
					a(zn),
					a(gt),
					a(Bn),
					a(Le),
					a(Dn),
					a($t),
					a(Nn),
					a(wt),
					a(Un),
					a(vt),
					a(Yn),
					a(_t),
					a(Kn),
					a(Ee),
					a(Fn),
					a(bt),
					a(Jn),
					a(q),
					a(Xn),
					a(He),
					a(Qn),
					a(yt),
					a(Zn),
					a(xt),
					a(Vn),
					a(It),
					a(eo),
					a(Ge),
					a(to),
					a(R),
					a(no),
					a(We),
					a(oo),
					a(Se),
					a(ao),
					a(At),
					a(so),
					a(Ct),
					a(lo),
					a(je),
					a(io),
					a(kt),
					a(ro),
					a(ze),
					a(uo),
					a(De),
					a(ho),
					a(j),
					a(fo),
					a(sn)),
					y(f),
					y(Q),
					y(Z),
					y(ee),
					y(te),
					y(oe),
					y(ae),
					y(se),
					y(ie),
					y(ue),
					y(he),
					y(me),
					y(pe),
					y(de),
					y(ge),
					y($e),
					y(ve),
					y(_e),
					y(ye),
					y(Ae),
					y(ke),
					y(Te),
					y(Pe),
					y(Me),
					y(Oe),
					y(qe),
					y(Re),
					y(Be),
					y(Ne),
					y(Ue),
					y(Ye)
			}
		}
	)
}
function ul(m) {
	let n, t
	const f = [m[0], As]
	let Qe = { $$slots: { default: [rl] }, $$scope: { ctx: m } }
	for (let I = 0; I < f.length; I += 1) Qe = xa(Qe, f[I])
	return (
		(n = new Ls({ props: Qe })),
		{
			c() {
				$(n.$$.fragment)
			},
			l(I) {
				w(n.$$.fragment, I)
			},
			m(I, A) {
				v(n, I, A), (t = !0)
			},
			p(I, [A]) {
				const hn = A & 1 ? Ps(f, [A & 1 && Is(I[0]), A & 0 && Is(As)]) : {}
				A & 2 && (hn.$$scope = { dirty: A, ctx: I }), n.$set(hn)
			},
			i(I) {
				t || (_(n.$$.fragment, I), (t = !0))
			},
			o(I) {
				b(n.$$.fragment, I), (t = !1)
			},
			d(I) {
				y(n, I)
			}
		}
	)
}
const As = {
	title: 'The existential risk of superintelligent AI',
	description:
		'Why AI is a risk for the future of our existence, and why we need to pause development.'
}
function hl(m, n, t) {
	return (
		(m.$$set = (f) => {
			t(0, (n = xa(xa({}, n), xs(f))))
		}),
		(n = xs(n)),
		[n]
	)
}
class gl extends ks {
	constructor(n) {
		super(), Ts(this, n, hl, ul, Cs, {})
	}
}
export { gl as default, As as metadata }
