import {
	s as Qn,
	a as ln,
	e as zn,
	c as p,
	b as h,
	t as i,
	f,
	g as m,
	d,
	h as H,
	i as r,
	j as s,
	k as Re,
	l as o,
	m as l,
	n as Xn
} from './scheduler.D9JQr37X.js'
import {
	S as Yn,
	i as Zn,
	c as $,
	b as v,
	m as b,
	a as _,
	t as w,
	d as y
} from './index.D-WnFt3a.js'
import { g as ea, a as Jn } from './a.svelte_svelte_type_style_lang.DfavE63L.js'
import { M as ta } from './mdsvex.Bi9EMyuJ.js'
import { A as I } from './a.YKMG9Usu.js'
function na(u) {
	let t
	return {
		c() {
			t = i('hack into systems')
		},
		l(n) {
			t = r(n, 'hack into systems')
		},
		m(n, c) {
			o(n, t, c)
		},
		d(n) {
			n && s(t)
		}
	}
}
function aa(u) {
	let t
	return {
		c() {
			t = i('state-of-the-art')
		},
		l(n) {
			t = r(n, 'state-of-the-art')
		},
		m(n, c) {
			o(n, t, c)
		},
		d(n) {
			n && s(t)
		}
	}
}
function sa(u) {
	let t
	return {
		c() {
			t = i('produce all the steps needed to create a new pandemic')
		},
		l(n) {
			t = r(n, 'produce all the steps needed to create a new pandemic')
		},
		m(n, c) {
			o(n, t, c)
		},
		d(n) {
			n && s(t)
		}
	}
}
function oa(u) {
	let t
	return {
		c() {
			t = i('40,000 new chemical weapons in six hours')
		},
		l(n) {
			t = r(n, '40,000 new chemical weapons in six hours')
		},
		m(n, c) {
			o(n, t, c)
		},
		d(n) {
			n && s(t)
		}
	}
}
function la(u) {
	let t
	return {
		c() {
			t = i('AlphaDev')
		},
		l(n) {
			t = r(n, 'AlphaDev')
		},
		m(n, c) {
			o(n, t, c)
		},
		d(n) {
			n && s(t)
		}
	}
}
function ia(u) {
	let t
	return {
		c() {
			t = i('already present')
		},
		l(n) {
			t = r(n, 'already present')
		},
		m(n, c) {
			o(n, t, c)
		},
		d(n) {
			n && s(t)
		}
	}
}
function ra(u) {
	let t,
		n = 'AI takeover'
	return {
		c() {
			;(t = p('em')), (t.textContent = n)
		},
		l(c) {
			;(t = f(c, 'EM', { 'data-svelte-h': !0 })), m(t) !== 'svelte-r5ovmf' && (t.textContent = n)
		},
		m(c, k) {
			o(c, t, k)
		},
		p: Xn,
		d(c) {
			c && s(t)
		}
	}
}
function ca(u) {
	let t
	return {
		c() {
			t = i('human extinction')
		},
		l(n) {
			t = r(n, 'human extinction')
		},
		m(n, c) {
			o(n, t, c)
		},
		d(n) {
			n && s(t)
		}
	}
}
function pa(u) {
	let t
	return {
		c() {
			t = i('RSP approach by Anthropic')
		},
		l(n) {
			t = r(n, 'RSP approach by Anthropic')
		},
		m(n, c) {
			o(n, t, c)
		},
		d(n) {
			n && s(t)
		}
	}
}
function fa(u) {
	let t
	return {
		c() {
			t = i('Coordinated Pausing')
		},
		l(n) {
			t = r(n, 'Coordinated Pausing')
		},
		m(n, c) {
			o(n, t, c)
		},
		d(n) {
			n && s(t)
		}
	}
}
function ua(u) {
	let t
	return {
		c() {
			t = i('level 2 regulation')
		},
		l(n) {
			t = r(n, 'level 2 regulation')
		},
		m(n, c) {
			o(n, t, c)
		},
		d(n) {
			n && s(t)
		}
	}
}
function ma(u) {
	let t
	return {
		c() {
			t = i('appending some specific words or characters to your chat message')
		},
		l(n) {
			t = r(n, 'appending some specific words or characters to your chat message')
		},
		m(n, c) {
			o(n, t, c)
		},
		d(n) {
			n && s(t)
		}
	}
}
function ha(u) {
	let t
	return {
		c() {
			t = i('creatively rephrasing your message')
		},
		l(n) {
			t = r(n, 'creatively rephrasing your message')
		},
		m(n, c) {
			o(n, t, c)
		},
		d(n) {
			n && s(t)
		}
	}
}
function da(u) {
	let t
	return {
		c() {
			t = i('unclear')
		},
		l(n) {
			t = r(n, 'unclear')
		},
		m(n, c) {
			o(n, t, c)
		},
		d(n) {
			n && s(t)
		}
	}
}
function ga(u) {
	let t
	return {
		c() {
			t = i('by over 50%')
		},
		l(n) {
			t = r(n, 'by over 50%')
		},
		m(n, c) {
			o(n, t, c)
		},
		d(n) {
			n && s(t)
		}
	}
}
function $a(u) {
	let t
	return {
		c() {
			t = i('Voyager')
		},
		l(n) {
			t = r(n, 'Voyager')
		},
		m(n, c) {
			o(n, t, c)
		},
		d(n) {
			n && s(t)
		}
	}
}
function va(u) {
	let t
	return {
		c() {
			t = i('takes over')
		},
		l(n) {
			t = r(n, 'takes over')
		},
		m(n, c) {
			o(n, t, c)
		},
		d(n) {
			n && s(t)
		}
	}
}
function ba(u) {
	let t
	return {
		c() {
			t = i(
				'Unfortunately, not a single draft proposal right now actually prevents or delays superintelligent AI.'
			)
		},
		l(n) {
			t = r(
				n,
				'Unfortunately, not a single draft proposal right now actually prevents or delays superintelligent AI.'
			)
		},
		m(n, c) {
			o(n, t, c)
		},
		d(n) {
			n && s(t)
		}
	}
}
function _a(u) {
	let t
	return {
		c() {
			t = i('calling for a Pause')
		},
		l(n) {
			t = r(n, 'calling for a Pause')
		},
		m(n, c) {
			o(n, t, c)
		},
		d(n) {
			n && s(t)
		}
	}
}
function wa(u) {
	let t,
		n = 'In this article, we’ll discuss:',
		c,
		k,
		A =
			'<li>Which AI capabilities can be dangerous</li> <li>How we can prevent these capabilities from appearing or spreading</li> <li>Why it is dangerous to rely on evaluations as a policy measure</li>',
		T,
		M,
		rn = `As AI models become more powerful and useful, they also become more dangerous.
So at which point should we proceed with caution?
One particular threshold that is often mentioned, is AGI - or Artificial General Intelligence.
There’s a lot of debate about what AGI exactly means.
Some say it’s when AI can do all the cognitive tasks that humans can.
Some say GPT-4 already is AGI.
Steve Wozniak defines AGI as the first system that can enter a kitchen and make a cup of coffee.`,
		ze,
		_e,
		cn = `From a safety perspective, the definition of AGI is not that important.
In fact, it can give us a false impression of safety, because we could think that we’re safe until we reach AGI.
Even if an AI can not make a cup of coffee, it could still be dangerous.
What matters is <em>which capabilities an AI has</em>.`,
		Je,
		we,
		pn =
			'In this article, we’ll dive into various dangerous capabilities, and what we can do to prevent them from actually harming us.',
		Ke,
		D,
		fn = 'Which capabilities can be dangerous?',
		Qe,
		C,
		E,
		Oe,
		un = 'Cybersecurity',
		wt,
		B,
		yt,
		U,
		At,
		xt,
		G,
		We,
		mn = 'Biological',
		It,
		V,
		Ct,
		z,
		kt,
		Tt,
		S,
		je,
		hn = 'Algorithmic improvements',
		Pt,
		qe,
		dn = 'intelligence explosion',
		Ht,
		J,
		Lt,
		Mt,
		K,
		Fe,
		gn = 'Deception',
		Et,
		Q,
		Gt,
		St,
		R,
		Ne,
		$n = 'Self-replication',
		Rt,
		X,
		Ot,
		Y,
		Wt,
		Xe,
		ye,
		vn =
			'This list is not exhaustive, so there are other dangerous capabilities that an AI could have.',
		Ye,
		Z,
		bn = 'Preventing creation of dangerous capabilities',
		Ze,
		Ae,
		_n = `Can we prevent these dangerous capabilities from appearing?
As AIs become larger and are trained on more data, they attain new abilities.
It turns out to be very hard to predict which abilities will appear, and how well an AI will perform.
Because of this, they are often called <em>Emergent Capabilities</em>.`,
		et,
		xe,
		wn = `Our current paradigm of large language models is almost inherently unpredictable.
AI models are not written like software - they are trained.
They are black boxes consisting of billions of numerical parameters.
No-one really knows what’s going on inside.
This unpredictability makes it hard to say whether a training run will result in a dangerous AI.
Interpretability research may change this in the future, but as of now, we can’t really explain why AI does what it does.`,
		tt,
		Ie,
		yn = `So preventing the creation of dangerous capabilities can practically only be done in one way:
don’t build increasingly powerful AI systems in the first place.
This would be the safest way forward, but that’s not what AI labs are proposing.
Preventing the proliferation of dangerous capabilities`,
		nt,
		g,
		jt,
		De,
		An = 'evaluations',
		qt,
		Be,
		xn = 'evals',
		Ft,
		ee,
		Nt,
		te,
		Dt,
		ne,
		Bt,
		Ue,
		In = 'being created',
		Ut,
		Ve,
		Cn = 'deployed',
		Vt,
		at,
		Ce,
		kn = `<li><strong>Models can be leaked</strong>.
We saw this happen with Meta’s LLAMA model. Once it’s out there, there is no going back.</li> <li><strong>Some capabilities are even dangerous inside AI labs</strong>.
A self-replicating AI, for example, could escape from the lab before deployment.</li> <li><strong>Testing for dangerous capabilities is difficult</strong>.
We don’t know how we can (safely) test if an AI can self-replicate, for example. Or how to test if it deceives humans</li> <li><strong>Capabilities can be added or discovered after training</strong>.
This includes fine-tuning, jailbreaking, and runtime improvements.</li>`,
		st,
		ke,
		Tn = 'We’ll dive into this last point in more detail.',
		ot,
		ae,
		Pn = 'Capabilities can be added after training',
		lt,
		se,
		Hn = 'Fine-tuning',
		it,
		Te,
		Ln = `Fine-tuning can be used to improve the capabilities of an existing AI model.
This is similar to training, but it’s much faster, much cheaper, doesn’t require as much data and can often be done on consumer hardware.
Fine-tuning changes the AIs parameters, and as such, changes its capabilities.
Now, fine-tuning is not as powerful as doing a full training run, but it can still improve on existing capabilities.`,
		rt,
		oe,
		Mn = 'Jailbreaking',
		ct,
		Pe,
		En = `The largest AIs are trained on absolutely vast amounts of data.
Most of the books, scientific articles, and websites on the internet.
There’s a lot of nasty stuff in these datasets.
AIs are often fine-tuned using a technique called RLHF (Reinforcement Learning from Human Feedback) to get them to be helpful and nice.
In this process the AI has to learn not to say certain things, like making racist remarks, explaining how to make a bomb or how to create a new bioweapon.`,
		pt,
		P,
		zt,
		le,
		Jt,
		ie,
		Kt,
		re,
		Qt,
		ft,
		ce,
		Gn = 'Runtime improvements',
		ut,
		He,
		Sn =
			'Runtime improvements make no changes to the model, but instead improve the way the model is used.',
		mt,
		pe,
		Xt,
		fe,
		Yt,
		ht,
		ue,
		Zt,
		me,
		en,
		dt,
		Le,
		Rn = `We don’t know how far a base model can be stretched.
Even if we stop training new AI models right now, we’ll probably see important innovations that add new capabilities to existing models.`,
		gt,
		he,
		On = 'In conclusion',
		$t,
		de,
		tn,
		ge,
		nn,
		vt,
		Me,
		Wn =
			'<li>Even if we test models before they are deployed, there are still ways in which they can get dangerous capabilities after deployment (fine-tuning, jailbreaking, runtime improvements).</li> <li>Models can be leaked.</li> <li>Some capabilities are even dangerous inside AI labs.</li>',
		bt,
		O,
		an,
		$e,
		sn,
		ve,
		on,
		_t
	return (
		(B = new I({
			props: { href: '/cybersecurity-risks', $$slots: { default: [na] }, $$scope: { ctx: u } }
		})),
		(U = new I({ props: { href: '/sota', $$slots: { default: [aa] }, $$scope: { ctx: u } } })),
		(V = new I({
			props: {
				href: 'https://arxiv.org/abs/2306.03809',
				rel: 'nofollow',
				$$slots: { default: [sa] },
				$$scope: { ctx: u }
			}
		})),
		(z = new I({
			props: {
				href: 'https://www.theverge.com/2022/3/17/22983197/ai-new-possible-chemical-weapons-generative-models-vx',
				rel: 'nofollow',
				$$slots: { default: [oa] },
				$$scope: { ctx: u }
			}
		})),
		(J = new I({
			props: {
				href: 'https://www.deepmind.com/blog/alphadev-discovers-faster-sorting-algorithms',
				rel: 'nofollow',
				$$slots: { default: [la] },
				$$scope: { ctx: u }
			}
		})),
		(Q = new I({
			props: {
				href: 'https://twitter.com/DanHendrycks/status/1699437800301752332',
				rel: 'nofollow',
				$$slots: { default: [ia] },
				$$scope: { ctx: u }
			}
		})),
		(X = new I({
			props: { href: '/ai-takeover', $$slots: { default: [ra] }, $$scope: { ctx: u } }
		})),
		(Y = new I({ props: { href: '/xrisk', $$slots: { default: [ca] }, $$scope: { ctx: u } } })),
		(ee = new I({
			props: {
				href: 'https://evals.alignment.org/blog/2023-09-26-rsp/#:~:text=An%20RSP%20specifies%20what%20level,capabilities%20until%20protective%20measures%20improve.',
				rel: 'nofollow',
				$$slots: { default: [pa] },
				$$scope: { ctx: u }
			}
		})),
		(te = new I({
			props: {
				href: 'https://www.governance.ai/research-paper/coordinated-pausing-evaluation-based-scheme',
				rel: 'nofollow',
				$$slots: { default: [fa] },
				$$scope: { ctx: u }
			}
		})),
		(ne = new I({
			props: { href: '/4-levels-of-ai-regulation', $$slots: { default: [ua] }, $$scope: { ctx: u } }
		})),
		(le = new I({
			props: {
				href: 'https://twitter.com/AIPanicLive/status/1678942758872989696',
				rel: 'nofollow',
				$$slots: { default: [ma] },
				$$scope: { ctx: u }
			}
		})),
		(ie = new I({
			props: {
				href: 'https://twitter.com/_annieversary/status/1647865782741749760',
				rel: 'nofollow',
				$$slots: { default: [ha] },
				$$scope: { ctx: u }
			}
		})),
		(re = new I({
			props: {
				href: 'https://llm-attacks.org/',
				rel: 'nofollow',
				$$slots: { default: [da] },
				$$scope: { ctx: u }
			}
		})),
		(fe = new I({
			props: {
				href: 'https://arxiv.org/pdf/2309.03409.pdf',
				rel: 'nofollow',
				$$slots: { default: [ga] },
				$$scope: { ctx: u }
			}
		})),
		(me = new I({
			props: {
				href: 'https://arxiv.org/abs/2305.16291',
				rel: 'nofollow',
				$$slots: { default: [$a] },
				$$scope: { ctx: u }
			}
		})),
		(ge = new I({
			props: { href: '/ai-takeover', $$slots: { default: [va] }, $$scope: { ctx: u } }
		})),
		($e = new I({
			props: {
				href: 'https://twitter.com/PauseAI/status/1704998018322141496',
				rel: 'nofollow',
				$$slots: { default: [ba] },
				$$scope: { ctx: u }
			}
		})),
		(ve = new I({ props: { href: '/proposal', $$slots: { default: [_a] }, $$scope: { ctx: u } } })),
		{
			c() {
				;(t = p('p')),
					(t.textContent = n),
					(c = h()),
					(k = p('ul')),
					(k.innerHTML = A),
					(T = h()),
					(M = p('p')),
					(M.textContent = rn),
					(ze = h()),
					(_e = p('p')),
					(_e.innerHTML = cn),
					(Je = h()),
					(we = p('p')),
					(we.textContent = pn),
					(Ke = h()),
					(D = p('h2')),
					(D.textContent = fn),
					(Qe = h()),
					(C = p('ul')),
					(E = p('li')),
					(Oe = p('strong')),
					(Oe.textContent = un),
					(wt = i(
						'. When an AI is able to discover security vulnerabilities (especially new, unknown ones), it can (be used to) '
					)),
					$(B.$$.fragment),
					(yt = i('. Current ')),
					$(U.$$.fragment),
					(At = i(
						' AI systems can find some security vulnerabilities, but not yet at dangerous, advanced levels. However, as cybersecurity capabilities increase, so does the potential damage an AI-assisted cyberweapon could do. Large scale cyberattacks could disrupt our infrastructure, disable payments and cause chaos.'
					)),
					(xt = h()),
					(G = p('li')),
					(We = p('strong')),
					(We.textContent = mn),
					(It = i(
						'. Design novel biological agents, or help in the process of engineering a pandemic. A group of students was able to use a chatbot to '
					)),
					$(V.$$.fragment),
					(Ct = i('. An AI designed to find safe medicine was used to discover ')),
					$(z.$$.fragment),
					(kt = i('.')),
					(Tt = h()),
					(S = p('li')),
					(je = p('strong')),
					(je.textContent = hn),
					(Pt = i(
						'. An AI that can find efficient algorithms for a given problem, could lead to a recursive loop of self-improvement, spinning rapidly out of control. This is called an '
					)),
					(qe = p('em')),
					(qe.textContent = dn),
					(Ht = i(
						'. The resulting AI would be incredibly powerful and could have all sorts of other dangerous capabilities. Luckily, no AI can self-improve yet. However, there are AIs that can find new, very efficient algorithms (like '
					)),
					$(J.$$.fragment),
					(Lt = i(').')),
					(Mt = h()),
					(K = p('li')),
					(Fe = p('strong')),
					(Fe.textContent = gn),
					(Et = i(
						'. The ability to manipulate people, which includes social engineering. Various forms of deception are '
					)),
					$(Q.$$.fragment),
					(Gt = i(
						' in current AI systems. For example, Meta’s CICERO AI (which was trained to lead to “Better, more natural AI-human cooperation”) turned out to an expert liar, deceiving other agents in the game. An AI that can deceive humans, may deceive humans during training runs. It could hide its capabilities or intentions.'
					)),
					(St = h()),
					(R = p('li')),
					(Ne = p('strong')),
					(Ne.textContent = $n),
					(Rt = i(
						'. If an AI can create new instances on other machines, there is a risk of it spreading uncontrollably, leading to an '
					)),
					$(X.$$.fragment),
					(Ot = i('. A sufficiently capable AI could outcompete humans and lead to ')),
					$(Y.$$.fragment),
					(Wt = i('. Note that this could even happen before an AI model is deployed.')),
					(Xe = h()),
					(ye = p('p')),
					(ye.textContent = vn),
					(Ye = h()),
					(Z = p('h2')),
					(Z.textContent = bn),
					(Ze = h()),
					(Ae = p('p')),
					(Ae.innerHTML = _n),
					(et = h()),
					(xe = p('p')),
					(xe.textContent = wn),
					(tt = h()),
					(Ie = p('p')),
					(Ie.textContent = yn),
					(nt = h()),
					(g = p('p')),
					(jt = i(`Right now, a lot is happening in the AI regulation space.
A lot of these proposals (including all the ones coming from AI labs) rely on safety `)),
					(De = p('strong')),
					(De.textContent = An),
					(qt = i(' (or ')),
					(Be = p('em')),
					(Be.textContent = xn),
					(Ft = i(`): pre-deployment testing of AI models.
An example of these eval-based approaches is the `)),
					$(ee.$$.fragment),
					(Nt = i(' or the ')),
					$(te.$$.fragment),
					(Dt = i(` approach by GovAI.
We refer to these as `)),
					$(ne.$$.fragment),
					(Bt = i(`.
These evaluations do not prevent dangerous AIs from `)),
					(Ue = p('em')),
					(Ue.textContent = In),
					(Ut = i(', but they do prevent them from being ')),
					(Ve = p('em')),
					(Ve.textContent = Cn),
					(Vt = i(`.
This type of policy is relatively cheap, and it still allows AI labs to continue their research.
However, we believe this approach is very dangerous:`)),
					(at = h()),
					(Ce = p('ul')),
					(Ce.innerHTML = kn),
					(st = h()),
					(ke = p('p')),
					(ke.textContent = Tn),
					(ot = h()),
					(ae = p('h2')),
					(ae.textContent = Pn),
					(lt = h()),
					(se = p('h3')),
					(se.textContent = Hn),
					(it = h()),
					(Te = p('p')),
					(Te.textContent = Ln),
					(rt = h()),
					(oe = p('h3')),
					(oe.textContent = Mn),
					(ct = h()),
					(Pe = p('p')),
					(Pe.textContent = En),
					(pt = h()),
					(P = p('p')),
					(zt = i(`But these safeguards are not perfect.
So-called “jailbreaking” is a technique where you try to get the AI to ignore these safeguards.
This can be done by `)),
					$(le.$$.fragment),
					(Jt = i(', or by ')),
					$(ie.$$.fragment),
					(Kt = i(`.
It is `)),
					$(re.$$.fragment),
					(Qt = i(' whether such behavior can ever be fully patched.')),
					(ft = h()),
					(ce = p('h3')),
					(ce.textContent = Gn),
					(ut = h()),
					(He = p('p')),
					(He.textContent = Sn),
					(mt = h()),
					(pe = p('p')),
					(Xt = i(`The simplest one of these, is to change the prompts.
Even small changes to prompts can have a large effect on the output of the model.
Adding a few words to a prompt can improve performance `)),
					$(fe.$$.fragment),
					(Yt = i('.')),
					(ht = h()),
					(ue = p('p')),
					(Zt = i(`But we can also use all sorts of software to augment some base model.
For example, people have found ways to add long-term memory to GPT-4, by letting the model query a database.
Or consider AutoGPT, which lets a model call itself recursively, which means it can run autonomously for any length of time.
Or consider `)),
					$(me.$$.fragment),
					(en = i(
						', a tool that enabled GPT-4 to play Minecraft fully autonomously. It even got to diamond gear.'
					)),
					(dt = h()),
					(Le = p('p')),
					(Le.textContent = Rn),
					(gt = h()),
					(he = p('h2')),
					(he.textContent = On),
					($t = h()),
					(de = p('p')),
					(tn = i(
						'Dangerous capabilities from AI can lead to all sorts of problems: large scale cyberattacks, engineered pandemics, and rogue AI that '
					)),
					$(ge.$$.fragment),
					(nn = i(`.
It is tempting to rely on evaluations to prevent these dangerous capabilities from appearing or spreading, but this is a dangerous approach:`)),
					(vt = h()),
					(Me = p('ul')),
					(Me.innerHTML = Wn),
					(bt = h()),
					(O = p('p')),
					(an =
						i(`The only safe option is to not build these powerful AI systems in the first place.
We should not allow the creation of these unpredictable, potentially highly dangerous AI systems.
`)),
					$($e.$$.fragment),
					(sn = i(`
That’s why we’re `)),
					$(ve.$$.fragment),
					(on = i('!')),
					this.h()
			},
			l(e) {
				;(t = f(e, 'P', { 'data-svelte-h': !0 })),
					m(t) !== 'svelte-wvj0gi' && (t.textContent = n),
					(c = d(e)),
					(k = f(e, 'UL', { 'data-svelte-h': !0 })),
					m(k) !== 'svelte-1yxv4pa' && (k.innerHTML = A),
					(T = d(e)),
					(M = f(e, 'P', { 'data-svelte-h': !0 })),
					m(M) !== 'svelte-cq20fp' && (M.textContent = rn),
					(ze = d(e)),
					(_e = f(e, 'P', { 'data-svelte-h': !0 })),
					m(_e) !== 'svelte-1cd4n22' && (_e.innerHTML = cn),
					(Je = d(e)),
					(we = f(e, 'P', { 'data-svelte-h': !0 })),
					m(we) !== 'svelte-258mgr' && (we.textContent = pn),
					(Ke = d(e)),
					(D = f(e, 'H2', { id: !0, 'data-svelte-h': !0 })),
					m(D) !== 'svelte-17rf2j7' && (D.textContent = fn),
					(Qe = d(e)),
					(C = f(e, 'UL', {}))
				var a = H(C)
				E = f(a, 'LI', {})
				var W = H(E)
				;(Oe = f(W, 'STRONG', { 'data-svelte-h': !0 })),
					m(Oe) !== 'svelte-lilg6r' && (Oe.textContent = un),
					(wt = r(
						W,
						'. When an AI is able to discover security vulnerabilities (especially new, unknown ones), it can (be used to) '
					)),
					v(B.$$.fragment, W),
					(yt = r(W, '. Current ')),
					v(U.$$.fragment, W),
					(At = r(
						W,
						' AI systems can find some security vulnerabilities, but not yet at dangerous, advanced levels. However, as cybersecurity capabilities increase, so does the potential damage an AI-assisted cyberweapon could do. Large scale cyberattacks could disrupt our infrastructure, disable payments and cause chaos.'
					)),
					W.forEach(s),
					(xt = d(a)),
					(G = f(a, 'LI', {}))
				var j = H(G)
				;(We = f(j, 'STRONG', { 'data-svelte-h': !0 })),
					m(We) !== 'svelte-cdmw39' && (We.textContent = mn),
					(It = r(
						j,
						'. Design novel biological agents, or help in the process of engineering a pandemic. A group of students was able to use a chatbot to '
					)),
					v(V.$$.fragment, j),
					(Ct = r(j, '. An AI designed to find safe medicine was used to discover ')),
					v(z.$$.fragment, j),
					(kt = r(j, '.')),
					j.forEach(s),
					(Tt = d(a)),
					(S = f(a, 'LI', {}))
				var q = H(S)
				;(je = f(q, 'STRONG', { 'data-svelte-h': !0 })),
					m(je) !== 'svelte-k3tbkq' && (je.textContent = hn),
					(Pt = r(
						q,
						'. An AI that can find efficient algorithms for a given problem, could lead to a recursive loop of self-improvement, spinning rapidly out of control. This is called an '
					)),
					(qe = f(q, 'EM', { 'data-svelte-h': !0 })),
					m(qe) !== 'svelte-1usc1si' && (qe.textContent = dn),
					(Ht = r(
						q,
						'. The resulting AI would be incredibly powerful and could have all sorts of other dangerous capabilities. Luckily, no AI can self-improve yet. However, there are AIs that can find new, very efficient algorithms (like '
					)),
					v(J.$$.fragment, q),
					(Lt = r(q, ').')),
					q.forEach(s),
					(Mt = d(a)),
					(K = f(a, 'LI', {}))
				var be = H(K)
				;(Fe = f(be, 'STRONG', { 'data-svelte-h': !0 })),
					m(Fe) !== 'svelte-13of9a7' && (Fe.textContent = gn),
					(Et = r(
						be,
						'. The ability to manipulate people, which includes social engineering. Various forms of deception are '
					)),
					v(Q.$$.fragment, be),
					(Gt = r(
						be,
						' in current AI systems. For example, Meta’s CICERO AI (which was trained to lead to “Better, more natural AI-human cooperation”) turned out to an expert liar, deceiving other agents in the game. An AI that can deceive humans, may deceive humans during training runs. It could hide its capabilities or intentions.'
					)),
					be.forEach(s),
					(St = d(a)),
					(R = f(a, 'LI', {}))
				var F = H(R)
				;(Ne = f(F, 'STRONG', { 'data-svelte-h': !0 })),
					m(Ne) !== 'svelte-kcijpx' && (Ne.textContent = $n),
					(Rt = r(
						F,
						'. If an AI can create new instances on other machines, there is a risk of it spreading uncontrollably, leading to an '
					)),
					v(X.$$.fragment, F),
					(Ot = r(F, '. A sufficiently capable AI could outcompete humans and lead to ')),
					v(Y.$$.fragment, F),
					(Wt = r(F, '. Note that this could even happen before an AI model is deployed.')),
					F.forEach(s),
					a.forEach(s),
					(Xe = d(e)),
					(ye = f(e, 'P', { 'data-svelte-h': !0 })),
					m(ye) !== 'svelte-19sk9b6' && (ye.textContent = vn),
					(Ye = d(e)),
					(Z = f(e, 'H2', { id: !0, 'data-svelte-h': !0 })),
					m(Z) !== 'svelte-6bozo0' && (Z.textContent = bn),
					(Ze = d(e)),
					(Ae = f(e, 'P', { 'data-svelte-h': !0 })),
					m(Ae) !== 'svelte-v5pi3u' && (Ae.innerHTML = _n),
					(et = d(e)),
					(xe = f(e, 'P', { 'data-svelte-h': !0 })),
					m(xe) !== 'svelte-16484i8' && (xe.textContent = wn),
					(tt = d(e)),
					(Ie = f(e, 'P', { 'data-svelte-h': !0 })),
					m(Ie) !== 'svelte-cpmu5w' && (Ie.textContent = yn),
					(nt = d(e)),
					(g = f(e, 'P', {}))
				var x = H(g)
				;(jt = r(
					x,
					`Right now, a lot is happening in the AI regulation space.
A lot of these proposals (including all the ones coming from AI labs) rely on safety `
				)),
					(De = f(x, 'STRONG', { 'data-svelte-h': !0 })),
					m(De) !== 'svelte-1s31wwv' && (De.textContent = An),
					(qt = r(x, ' (or ')),
					(Be = f(x, 'EM', { 'data-svelte-h': !0 })),
					m(Be) !== 'svelte-1j2fhan' && (Be.textContent = xn),
					(Ft = r(
						x,
						`): pre-deployment testing of AI models.
An example of these eval-based approaches is the `
					)),
					v(ee.$$.fragment, x),
					(Nt = r(x, ' or the ')),
					v(te.$$.fragment, x),
					(Dt = r(
						x,
						` approach by GovAI.
We refer to these as `
					)),
					v(ne.$$.fragment, x),
					(Bt = r(
						x,
						`.
These evaluations do not prevent dangerous AIs from `
					)),
					(Ue = f(x, 'EM', { 'data-svelte-h': !0 })),
					m(Ue) !== 'svelte-d02mjv' && (Ue.textContent = In),
					(Ut = r(x, ', but they do prevent them from being ')),
					(Ve = f(x, 'EM', { 'data-svelte-h': !0 })),
					m(Ve) !== 'svelte-b8ejvo' && (Ve.textContent = Cn),
					(Vt = r(
						x,
						`.
This type of policy is relatively cheap, and it still allows AI labs to continue their research.
However, we believe this approach is very dangerous:`
					)),
					x.forEach(s),
					(at = d(e)),
					(Ce = f(e, 'UL', { 'data-svelte-h': !0 })),
					m(Ce) !== 'svelte-108f7q2' && (Ce.innerHTML = kn),
					(st = d(e)),
					(ke = f(e, 'P', { 'data-svelte-h': !0 })),
					m(ke) !== 'svelte-1dfuofk' && (ke.textContent = Tn),
					(ot = d(e)),
					(ae = f(e, 'H2', { id: !0, 'data-svelte-h': !0 })),
					m(ae) !== 'svelte-17q0pkn' && (ae.textContent = Pn),
					(lt = d(e)),
					(se = f(e, 'H3', { id: !0, 'data-svelte-h': !0 })),
					m(se) !== 'svelte-1mpeoh0' && (se.textContent = Hn),
					(it = d(e)),
					(Te = f(e, 'P', { 'data-svelte-h': !0 })),
					m(Te) !== 'svelte-jt8nbf' && (Te.textContent = Ln),
					(rt = d(e)),
					(oe = f(e, 'H3', { id: !0, 'data-svelte-h': !0 })),
					m(oe) !== 'svelte-107un54' && (oe.textContent = Mn),
					(ct = d(e)),
					(Pe = f(e, 'P', { 'data-svelte-h': !0 })),
					m(Pe) !== 'svelte-69srpk' && (Pe.textContent = En),
					(pt = d(e)),
					(P = f(e, 'P', {}))
				var L = H(P)
				;(zt = r(
					L,
					`But these safeguards are not perfect.
So-called “jailbreaking” is a technique where you try to get the AI to ignore these safeguards.
This can be done by `
				)),
					v(le.$$.fragment, L),
					(Jt = r(L, ', or by ')),
					v(ie.$$.fragment, L),
					(Kt = r(
						L,
						`.
It is `
					)),
					v(re.$$.fragment, L),
					(Qt = r(L, ' whether such behavior can ever be fully patched.')),
					L.forEach(s),
					(ft = d(e)),
					(ce = f(e, 'H3', { id: !0, 'data-svelte-h': !0 })),
					m(ce) !== 'svelte-17yey2n' && (ce.textContent = Gn),
					(ut = d(e)),
					(He = f(e, 'P', { 'data-svelte-h': !0 })),
					m(He) !== 'svelte-pn0ztw' && (He.textContent = Sn),
					(mt = d(e)),
					(pe = f(e, 'P', {}))
				var Ee = H(pe)
				;(Xt = r(
					Ee,
					`The simplest one of these, is to change the prompts.
Even small changes to prompts can have a large effect on the output of the model.
Adding a few words to a prompt can improve performance `
				)),
					v(fe.$$.fragment, Ee),
					(Yt = r(Ee, '.')),
					Ee.forEach(s),
					(ht = d(e)),
					(ue = f(e, 'P', {}))
				var Ge = H(ue)
				;(Zt = r(
					Ge,
					`But we can also use all sorts of software to augment some base model.
For example, people have found ways to add long-term memory to GPT-4, by letting the model query a database.
Or consider AutoGPT, which lets a model call itself recursively, which means it can run autonomously for any length of time.
Or consider `
				)),
					v(me.$$.fragment, Ge),
					(en = r(
						Ge,
						', a tool that enabled GPT-4 to play Minecraft fully autonomously. It even got to diamond gear.'
					)),
					Ge.forEach(s),
					(dt = d(e)),
					(Le = f(e, 'P', { 'data-svelte-h': !0 })),
					m(Le) !== 'svelte-dnkgdw' && (Le.textContent = Rn),
					(gt = d(e)),
					(he = f(e, 'H2', { id: !0, 'data-svelte-h': !0 })),
					m(he) !== 'svelte-as017' && (he.textContent = On),
					($t = d(e)),
					(de = f(e, 'P', {}))
				var Se = H(de)
				;(tn = r(
					Se,
					'Dangerous capabilities from AI can lead to all sorts of problems: large scale cyberattacks, engineered pandemics, and rogue AI that '
				)),
					v(ge.$$.fragment, Se),
					(nn = r(
						Se,
						`.
It is tempting to rely on evaluations to prevent these dangerous capabilities from appearing or spreading, but this is a dangerous approach:`
					)),
					Se.forEach(s),
					(vt = d(e)),
					(Me = f(e, 'UL', { 'data-svelte-h': !0 })),
					m(Me) !== 'svelte-1od6lri' && (Me.innerHTML = Wn),
					(bt = d(e)),
					(O = f(e, 'P', {}))
				var N = H(O)
				;(an = r(
					N,
					`The only safe option is to not build these powerful AI systems in the first place.
We should not allow the creation of these unpredictable, potentially highly dangerous AI systems.
`
				)),
					v($e.$$.fragment, N),
					(sn = r(
						N,
						`
That’s why we’re `
					)),
					v(ve.$$.fragment, N),
					(on = r(N, '!')),
					N.forEach(s),
					this.h()
			},
			h() {
				Re(D, 'id', 'which-capabilities-can-be-dangerous'),
					Re(Z, 'id', 'preventing-creation-of-dangerous-capabilities'),
					Re(ae, 'id', 'capabilities-can-be-added-after-training'),
					Re(se, 'id', 'fine-tuning'),
					Re(oe, 'id', 'jailbreaking'),
					Re(ce, 'id', 'runtime-improvements'),
					Re(he, 'id', 'in-conclusion')
			},
			m(e, a) {
				o(e, t, a),
					o(e, c, a),
					o(e, k, a),
					o(e, T, a),
					o(e, M, a),
					o(e, ze, a),
					o(e, _e, a),
					o(e, Je, a),
					o(e, we, a),
					o(e, Ke, a),
					o(e, D, a),
					o(e, Qe, a),
					o(e, C, a),
					l(C, E),
					l(E, Oe),
					l(E, wt),
					b(B, E, null),
					l(E, yt),
					b(U, E, null),
					l(E, At),
					l(C, xt),
					l(C, G),
					l(G, We),
					l(G, It),
					b(V, G, null),
					l(G, Ct),
					b(z, G, null),
					l(G, kt),
					l(C, Tt),
					l(C, S),
					l(S, je),
					l(S, Pt),
					l(S, qe),
					l(S, Ht),
					b(J, S, null),
					l(S, Lt),
					l(C, Mt),
					l(C, K),
					l(K, Fe),
					l(K, Et),
					b(Q, K, null),
					l(K, Gt),
					l(C, St),
					l(C, R),
					l(R, Ne),
					l(R, Rt),
					b(X, R, null),
					l(R, Ot),
					b(Y, R, null),
					l(R, Wt),
					o(e, Xe, a),
					o(e, ye, a),
					o(e, Ye, a),
					o(e, Z, a),
					o(e, Ze, a),
					o(e, Ae, a),
					o(e, et, a),
					o(e, xe, a),
					o(e, tt, a),
					o(e, Ie, a),
					o(e, nt, a),
					o(e, g, a),
					l(g, jt),
					l(g, De),
					l(g, qt),
					l(g, Be),
					l(g, Ft),
					b(ee, g, null),
					l(g, Nt),
					b(te, g, null),
					l(g, Dt),
					b(ne, g, null),
					l(g, Bt),
					l(g, Ue),
					l(g, Ut),
					l(g, Ve),
					l(g, Vt),
					o(e, at, a),
					o(e, Ce, a),
					o(e, st, a),
					o(e, ke, a),
					o(e, ot, a),
					o(e, ae, a),
					o(e, lt, a),
					o(e, se, a),
					o(e, it, a),
					o(e, Te, a),
					o(e, rt, a),
					o(e, oe, a),
					o(e, ct, a),
					o(e, Pe, a),
					o(e, pt, a),
					o(e, P, a),
					l(P, zt),
					b(le, P, null),
					l(P, Jt),
					b(ie, P, null),
					l(P, Kt),
					b(re, P, null),
					l(P, Qt),
					o(e, ft, a),
					o(e, ce, a),
					o(e, ut, a),
					o(e, He, a),
					o(e, mt, a),
					o(e, pe, a),
					l(pe, Xt),
					b(fe, pe, null),
					l(pe, Yt),
					o(e, ht, a),
					o(e, ue, a),
					l(ue, Zt),
					b(me, ue, null),
					l(ue, en),
					o(e, dt, a),
					o(e, Le, a),
					o(e, gt, a),
					o(e, he, a),
					o(e, $t, a),
					o(e, de, a),
					l(de, tn),
					b(ge, de, null),
					l(de, nn),
					o(e, vt, a),
					o(e, Me, a),
					o(e, bt, a),
					o(e, O, a),
					l(O, an),
					b($e, O, null),
					l(O, sn),
					b(ve, O, null),
					l(O, on),
					(_t = !0)
			},
			p(e, a) {
				const W = {}
				a & 2 && (W.$$scope = { dirty: a, ctx: e }), B.$set(W)
				const j = {}
				a & 2 && (j.$$scope = { dirty: a, ctx: e }), U.$set(j)
				const q = {}
				a & 2 && (q.$$scope = { dirty: a, ctx: e }), V.$set(q)
				const be = {}
				a & 2 && (be.$$scope = { dirty: a, ctx: e }), z.$set(be)
				const F = {}
				a & 2 && (F.$$scope = { dirty: a, ctx: e }), J.$set(F)
				const x = {}
				a & 2 && (x.$$scope = { dirty: a, ctx: e }), Q.$set(x)
				const L = {}
				a & 2 && (L.$$scope = { dirty: a, ctx: e }), X.$set(L)
				const Ee = {}
				a & 2 && (Ee.$$scope = { dirty: a, ctx: e }), Y.$set(Ee)
				const Ge = {}
				a & 2 && (Ge.$$scope = { dirty: a, ctx: e }), ee.$set(Ge)
				const Se = {}
				a & 2 && (Se.$$scope = { dirty: a, ctx: e }), te.$set(Se)
				const N = {}
				a & 2 && (N.$$scope = { dirty: a, ctx: e }), ne.$set(N)
				const jn = {}
				a & 2 && (jn.$$scope = { dirty: a, ctx: e }), le.$set(jn)
				const qn = {}
				a & 2 && (qn.$$scope = { dirty: a, ctx: e }), ie.$set(qn)
				const Fn = {}
				a & 2 && (Fn.$$scope = { dirty: a, ctx: e }), re.$set(Fn)
				const Nn = {}
				a & 2 && (Nn.$$scope = { dirty: a, ctx: e }), fe.$set(Nn)
				const Dn = {}
				a & 2 && (Dn.$$scope = { dirty: a, ctx: e }), me.$set(Dn)
				const Bn = {}
				a & 2 && (Bn.$$scope = { dirty: a, ctx: e }), ge.$set(Bn)
				const Un = {}
				a & 2 && (Un.$$scope = { dirty: a, ctx: e }), $e.$set(Un)
				const Vn = {}
				a & 2 && (Vn.$$scope = { dirty: a, ctx: e }), ve.$set(Vn)
			},
			i(e) {
				_t ||
					(_(B.$$.fragment, e),
					_(U.$$.fragment, e),
					_(V.$$.fragment, e),
					_(z.$$.fragment, e),
					_(J.$$.fragment, e),
					_(Q.$$.fragment, e),
					_(X.$$.fragment, e),
					_(Y.$$.fragment, e),
					_(ee.$$.fragment, e),
					_(te.$$.fragment, e),
					_(ne.$$.fragment, e),
					_(le.$$.fragment, e),
					_(ie.$$.fragment, e),
					_(re.$$.fragment, e),
					_(fe.$$.fragment, e),
					_(me.$$.fragment, e),
					_(ge.$$.fragment, e),
					_($e.$$.fragment, e),
					_(ve.$$.fragment, e),
					(_t = !0))
			},
			o(e) {
				w(B.$$.fragment, e),
					w(U.$$.fragment, e),
					w(V.$$.fragment, e),
					w(z.$$.fragment, e),
					w(J.$$.fragment, e),
					w(Q.$$.fragment, e),
					w(X.$$.fragment, e),
					w(Y.$$.fragment, e),
					w(ee.$$.fragment, e),
					w(te.$$.fragment, e),
					w(ne.$$.fragment, e),
					w(le.$$.fragment, e),
					w(ie.$$.fragment, e),
					w(re.$$.fragment, e),
					w(fe.$$.fragment, e),
					w(me.$$.fragment, e),
					w(ge.$$.fragment, e),
					w($e.$$.fragment, e),
					w(ve.$$.fragment, e),
					(_t = !1)
			},
			d(e) {
				e &&
					(s(t),
					s(c),
					s(k),
					s(T),
					s(M),
					s(ze),
					s(_e),
					s(Je),
					s(we),
					s(Ke),
					s(D),
					s(Qe),
					s(C),
					s(Xe),
					s(ye),
					s(Ye),
					s(Z),
					s(Ze),
					s(Ae),
					s(et),
					s(xe),
					s(tt),
					s(Ie),
					s(nt),
					s(g),
					s(at),
					s(Ce),
					s(st),
					s(ke),
					s(ot),
					s(ae),
					s(lt),
					s(se),
					s(it),
					s(Te),
					s(rt),
					s(oe),
					s(ct),
					s(Pe),
					s(pt),
					s(P),
					s(ft),
					s(ce),
					s(ut),
					s(He),
					s(mt),
					s(pe),
					s(ht),
					s(ue),
					s(dt),
					s(Le),
					s(gt),
					s(he),
					s($t),
					s(de),
					s(vt),
					s(Me),
					s(bt),
					s(O)),
					y(B),
					y(U),
					y(V),
					y(z),
					y(J),
					y(Q),
					y(X),
					y(Y),
					y(ee),
					y(te),
					y(ne),
					y(le),
					y(ie),
					y(re),
					y(fe),
					y(me),
					y(ge),
					y($e),
					y(ve)
			}
		}
	)
}
function ya(u) {
	let t, n
	const c = [u[0], Kn]
	let k = { $$slots: { default: [wa] }, $$scope: { ctx: u } }
	for (let A = 0; A < c.length; A += 1) k = ln(k, c[A])
	return (
		(t = new ta({ props: k })),
		{
			c() {
				$(t.$$.fragment)
			},
			l(A) {
				v(t.$$.fragment, A)
			},
			m(A, T) {
				b(t, A, T), (n = !0)
			},
			p(A, [T]) {
				const M = T & 1 ? ea(c, [T & 1 && Jn(A[0]), T & 0 && Jn(Kn)]) : {}
				T & 2 && (M.$$scope = { dirty: T, ctx: A }), t.$set(M)
			},
			i(A) {
				n || (_(t.$$.fragment, A), (n = !0))
			},
			o(A) {
				w(t.$$.fragment, A), (n = !1)
			},
			d(A) {
				y(t, A)
			}
		}
	)
}
const Kn = {
	title: 'Regulating dangerous capabilities in AI',
	description:
		'The more powerful AI becomes in specific domains, the larger the risks become. How do we prevent these dangerous capabilities from appearing or spreading?'
}
function Aa(u, t, n) {
	return (
		(u.$$set = (c) => {
			n(0, (t = ln(ln({}, t), zn(c))))
		}),
		(t = zn(t)),
		[t]
	)
}
class Pa extends Yn {
	constructor(t) {
		super(), Zn(this, t, Aa, ya, Qn, {})
	}
}
export { Pa as default, Kn as metadata }
