import {
	s as Ot,
	a as yt,
	e as St,
	c as p,
	b as x,
	t as u,
	f,
	g as E,
	d as y,
	h as c,
	j as r,
	i as m,
	k as Ct,
	l as h,
	m as a,
	n as R
} from './scheduler.D9JQr37X.js'
import {
	S as Gt,
	i as Tt,
	c as _,
	b as v,
	m as w,
	a as b,
	t as I,
	d as A
} from './index.D-WnFt3a.js'
import { g as zt, a as Et } from './a.svelte_svelte_type_style_lang.DfavE63L.js'
import { M as Lt } from './mdsvex.Bi9EMyuJ.js'
import { A as k } from './a.YKMG9Usu.js'
function Ut(l) {
	let e
	return {
		c() {
			e = u('AI researchers, AIImpacts 2022')
		},
		l(t) {
			e = m(t, 'AI researchers, AIImpacts 2022')
		},
		m(t, n) {
			h(t, e, n)
		},
		d(t) {
			t && r(e)
		}
	}
}
function Nt(l) {
	let e
	return {
		c() {
			e = u('AI researchers, AIImpacts 2023')
		},
		l(t) {
			e = m(t, 'AI researchers, AIImpacts 2023')
		},
		m(t, n) {
			h(t, e, n)
		},
		d(t) {
			t && r(e)
		}
	}
}
function Pt(l) {
	let e
	return {
		c() {
			e = u('AI engineers / startup founders, State of AI Engineering')
		},
		l(t) {
			e = m(t, 'AI engineers / startup founders, State of AI Engineering')
		},
		m(t, n) {
			h(t, e, n)
		},
		d(t) {
			t && r(e)
		}
	}
}
function qt(l) {
	let e
	return {
		c() {
			e = u('p(doom)')
		},
		l(t) {
			e = m(t, 'p(doom)')
		},
		m(t, n) {
			h(t, e, n)
		},
		d(t) {
			t && r(e)
		}
	}
}
function Kt(l) {
	let e
	return {
		c() {
			e = u('AI safety researchers, AlignmentForum')
		},
		l(t) {
			e = m(t, 'AI safety researchers, AlignmentForum')
		},
		m(t, n) {
			h(t, e, n)
		},
		d(t) {
			t && r(e)
		}
	}
}
function jt(l) {
	let e
	return {
		c() {
			e = u('UK citizens, PublicFirst')
		},
		l(t) {
			e = m(t, 'UK citizens, PublicFirst')
		},
		m(t, n) {
			h(t, e, n)
		},
		d(t) {
			t && r(e)
		}
	}
}
function Ft(l) {
	let e
	return {
		c() {
			e = u('German citizens, Kira')
		},
		l(t) {
			e = m(t, 'German citizens, Kira')
		},
		m(t, n) {
			h(t, e, n)
		},
		d(t) {
			t && r(e)
		}
	}
}
function Yt(l) {
	let e
	return {
		c() {
			e = u('US citizens, RethinkPriorities')
		},
		l(t) {
			e = m(t, 'US citizens, RethinkPriorities')
		},
		m(t, n) {
			h(t, e, n)
		},
		d(t) {
			t && r(e)
		}
	}
}
function Dt(l) {
	let e
	return {
		c() {
			e = u('Australian citizens, Ready Research')
		},
		l(t) {
			e = m(t, 'Australian citizens, Ready Research')
		},
		m(t, n) {
			h(t, e, n)
		},
		d(t) {
			t && r(e)
		}
	}
}
function Ht(l) {
	let e,
		t = 'US citizens, RethinkPriorities'
	return {
		c() {
			;(e = p('strong')), (e.textContent = t)
		},
		l(n) {
			;(e = f(n, 'STRONG', { 'data-svelte-h': !0 })), E(e) !== 'svelte-tg4no' && (e.textContent = t)
		},
		m(n, i) {
			h(n, e, i)
		},
		p: R,
		d(n) {
			n && r(e)
		}
	}
}
function Jt(l) {
	let e,
		t = 'US citizens, YouGov'
	return {
		c() {
			;(e = p('strong')), (e.textContent = t)
		},
		l(n) {
			;(e = f(n, 'STRONG', { 'data-svelte-h': !0 })),
				E(e) !== 'svelte-x2s0zi' && (e.textContent = t)
		},
		m(n, i) {
			h(n, e, i)
		},
		p: R,
		d(n) {
			n && r(e)
		}
	}
}
function Mt(l) {
	let e,
		t = 'US citizens, YouGov'
	return {
		c() {
			;(e = p('strong')), (e.textContent = t)
		},
		l(n) {
			;(e = f(n, 'STRONG', { 'data-svelte-h': !0 })),
				E(e) !== 'svelte-x2s0zi' && (e.textContent = t)
		},
		m(n, i) {
			h(n, e, i)
		},
		p: R,
		d(n) {
			n && r(e)
		}
	}
}
function Qt(l) {
	let e,
		t = 'US citizens, AIPI'
	return {
		c() {
			;(e = p('strong')), (e.textContent = t)
		},
		l(n) {
			;(e = f(n, 'STRONG', { 'data-svelte-h': !0 })),
				E(e) !== 'svelte-16tj5tu' && (e.textContent = t)
		},
		m(n, i) {
			h(n, e, i)
		},
		p: R,
		d(n) {
			n && r(e)
		}
	}
}
function Bt(l) {
	let e,
		t = 'US CS professors, Axios Generation Lab'
	return {
		c() {
			;(e = p('strong')), (e.textContent = t)
		},
		l(n) {
			;(e = f(n, 'STRONG', { 'data-svelte-h': !0 })),
				E(e) !== 'svelte-12hxm7j' && (e.textContent = t)
		},
		m(n, i) {
			h(n, e, i)
		},
		p: R,
		d(n) {
			n && r(e)
		}
	}
}
function Vt(l) {
	let e,
		t = 'US citizens, Sentience Institute'
	return {
		c() {
			;(e = p('strong')), (e.textContent = t)
		},
		l(n) {
			;(e = f(n, 'STRONG', { 'data-svelte-h': !0 })),
				E(e) !== 'svelte-1ixq1ya' && (e.textContent = t)
		},
		m(n, i) {
			h(n, e, i)
		},
		p: R,
		d(n) {
			n && r(e)
		}
	}
}
function Wt(l) {
	let e,
		t = 'UK citizens, YouGov'
	return {
		c() {
			;(e = p('strong')), (e.textContent = t)
		},
		l(n) {
			;(e = f(n, 'STRONG', { 'data-svelte-h': !0 })),
				E(e) !== 'svelte-1nan6qe' && (e.textContent = t)
		},
		m(n, i) {
			h(n, e, i)
		},
		p: R,
		d(n) {
			n && r(e)
		}
	}
}
function Xt(l) {
	let e,
		t = 'UK citizens, AISCC'
	return {
		c() {
			;(e = p('strong')), (e.textContent = t)
		},
		l(n) {
			;(e = f(n, 'STRONG', { 'data-svelte-h': !0 })),
				E(e) !== 'svelte-1iatp6' && (e.textContent = t)
		},
		m(n, i) {
			h(n, e, i)
		},
		p: R,
		d(n) {
			n && r(e)
		}
	}
}
function Zt(l) {
	let e,
		t = 'NL, US, UK Citizens, Existential Risk Observatory'
	return {
		c() {
			;(e = p('strong')), (e.textContent = t)
		},
		l(n) {
			;(e = f(n, 'STRONG', { 'data-svelte-h': !0 })),
				E(e) !== 'svelte-2gdz80' && (e.textContent = t)
		},
		m(n, i) {
			h(n, e, i)
		},
		p: R,
		d(n) {
			n && r(e)
		}
	}
}
function en(l) {
	let e,
		t = 'Catastrophic risks from AI',
		n,
		i,
		$,
		S,
		C,
		Pe,
		qe,
		W,
		we,
		O,
		Ke,
		je,
		G,
		be,
		T,
		Fe,
		z,
		Ye,
		De,
		X,
		Ie,
		L,
		He,
		Je,
		Z,
		Ae,
		U,
		Me,
		Qe,
		ee,
		xe,
		N,
		Be,
		Ve,
		te,
		ye,
		P,
		We,
		Xe,
		ne,
		ke,
		q,
		Ze,
		Le,
		K,
		kt = 'Regulations & governance',
		Ue,
		d,
		oe,
		j,
		et,
		tt,
		se,
		F,
		nt,
		ot,
		ae,
		Y,
		st,
		at,
		re,
		D,
		rt,
		lt,
		le,
		H,
		it,
		pt,
		ie,
		J,
		ft,
		ct,
		pe,
		M,
		ut,
		mt,
		fe,
		Q,
		ht,
		$t,
		ce,
		B,
		dt,
		Ne
	return (
		(C = new k({
			props: {
				href: 'https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/',
				rel: 'nofollow',
				$$slots: { default: [Ut] },
				$$scope: { ctx: l }
			}
		})),
		(O = new k({
			props: {
				href: 'https://wiki.aiimpacts.org/ai_timelines/predictions_of_human-level_ai_timelines/ai_timeline_surveys/2023_expert_survey_on_progress_in_ai',
				rel: 'nofollow',
				$$slots: { default: [Nt] },
				$$scope: { ctx: l }
			}
		})),
		(T = new k({
			props: {
				href: 'https://elemental-croissant-32a.notion.site/State-of-AI-Engineering-2023-20c09dc1767f45988ee1f479b4a84135#694f89e86f9148cb855220ec05e9c631',
				rel: 'nofollow',
				$$slots: { default: [Pt] },
				$$scope: { ctx: l }
			}
		})),
		(z = new k({ props: { href: '/pdoom', $$slots: { default: [qt] }, $$scope: { ctx: l } } })),
		(L = new k({
			props: {
				href: 'https://web.archive.org/web/20221013014859/https://www.alignmentforum.org/posts/QvwSr5LsxyDeaPK5s/existential-risk-from-ai-survey-results',
				rel: 'nofollow',
				$$slots: { default: [Kt] },
				$$scope: { ctx: l }
			}
		})),
		(U = new k({
			props: {
				href: 'https://publicfirst.co.uk/ai/',
				rel: 'nofollow',
				$$slots: { default: [jt] },
				$$scope: { ctx: l }
			}
		})),
		(N = new k({
			props: {
				href: 'https://www.zeit.de/digital/2023-04/ki-risiken-angst-umfrage-forschung-kira',
				rel: 'nofollow',
				$$slots: { default: [Ft] },
				$$scope: { ctx: l }
			}
		})),
		(P = new k({
			props: {
				href: 'https://rethinkpriorities.org/publications/us-public-perception-of-cais-statement-and-the-risk-of-extinction',
				rel: 'nofollow',
				$$slots: { default: [Yt] },
				$$scope: { ctx: l }
			}
		})),
		(q = new k({
			props: {
				href: 'https://theconversation.com/80-of-australians-think-ai-risk-is-a-global-priority-the-government-needs-to-step-up-225175',
				rel: 'nofollow',
				$$slots: { default: [Dt] },
				$$scope: { ctx: l }
			}
		})),
		(j = new k({
			props: {
				href: 'https://forum.effectivealtruism.org/posts/ConFiY9cRmg37fs2p/us-public-opinion-of-ai-policy-and-risk',
				rel: 'nofollow',
				$$slots: { default: [Ht] },
				$$scope: { ctx: l }
			}
		})),
		(F = new k({
			props: {
				href: 'https://www.vox.com/future-perfect/2023/8/18/23836362/ai-slow-down-poll-regulation',
				rel: 'nofollow',
				$$slots: { default: [Jt] },
				$$scope: { ctx: l }
			}
		})),
		(Y = new k({
			props: {
				href: 'https://theaipi.org/poll-shows-voters-oppose-open-sourcing-ai-models-support-regulatory-representation-on-boards-and-say-ai-risks-outweigh-benefits-2/',
				rel: 'nofollow',
				$$slots: { default: [Mt] },
				$$scope: { ctx: l }
			}
		})),
		(D = new k({
			props: {
				href: 'https://www.politico.com/newsletters/digital-future-daily/2023/11/29/exclusive-what-people-actually-think-about-ai-00129147',
				rel: 'nofollow',
				$$slots: { default: [Qt] },
				$$scope: { ctx: l }
			}
		})),
		(H = new k({
			props: {
				href: 'https://www.axios.com/2023/09/05/ai-regulations-expert-survey',
				rel: 'nofollow',
				$$slots: { default: [Bt] },
				$$scope: { ctx: l }
			}
		})),
		(J = new k({
			props: {
				href: 'https://www.sentienceinstitute.org/aims-survey-supplement-2023',
				rel: 'nofollow',
				$$slots: { default: [Vt] },
				$$scope: { ctx: l }
			}
		})),
		(M = new k({
			props: {
				href: 'https://inews.co.uk/news/politics/voters-deepfakes-ban-ai-intelligent-humans-2708693',
				rel: 'nofollow',
				$$slots: { default: [Wt] },
				$$scope: { ctx: l }
			}
		})),
		(Q = new k({
			props: {
				href: 'https://aiscc.org/2023/11/01/yougov-poll-83-of-brits-demand-companies-prove-ai-systems-are-safe-before-release/',
				rel: 'nofollow',
				$$slots: { default: [Xt] },
				$$scope: { ctx: l }
			}
		})),
		(B = new k({
			props: {
				href: 'https://www.existentialriskobservatory.org/papers_and_reports/Trends%20in%20Public%20Attitude%20Towards%20Existential%20Risk%20And%20Artificial%20Intelligence.pdf',
				rel: 'nofollow',
				$$slots: { default: [Zt] },
				$$scope: { ctx: l }
			}
		})),
		{
			c() {
				;(e = p('h2')),
					(e.textContent = t),
					(n = x()),
					(i = p('ul')),
					($ = p('li')),
					(S = p('strong')),
					_(C.$$.fragment),
					(Pe = u(
						': give “really bad outcomes (such as human extinction)” a 14% probability, with a median of 5%. 82% believe the control problem is important.'
					)),
					(qe = x()),
					(W = p('li')),
					(we = p('strong')),
					_(O.$$.fragment),
					(Ke = u(
						': Average p(doom) between 14 and 19.4%, depending on how the question of phrased. 86% believe the control problem is important.'
					)),
					(je = x()),
					(G = p('li')),
					(be = p('strong')),
					_(T.$$.fragment),
					(Fe = u(': over 60% have a ')),
					_(z.$$.fragment),
					(Ye = u(' > 25%. Only 12% have a p(doom) = 0.')),
					(De = x()),
					(X = p('li')),
					(Ie = p('strong')),
					_(L.$$.fragment),
					(He = u(
						': respondents assigned a median probability of 20% to x-risk caused due to a lack of enough technical research, and 30% to x-risk caused due to a failure of AI systems to do what the people deploying them intended, with huge variation (for example, there are data points at both ~1% and ~99%).'
					)),
					(Je = x()),
					(Z = p('li')),
					(Ae = p('strong')),
					_(U.$$.fragment),
					(Me = u(
						': think there’s a 9% probability humans will go extinct because of AI. About 50% of say they’re very or somewhat worried about this.'
					)),
					(Qe = x()),
					(ee = p('li')),
					(xe = p('strong')),
					_(N.$$.fragment),
					(Be = u(
						': Only 14% believe AI will have a positive influence on the world, 40% mixed, 40% negative.'
					)),
					(Ve = x()),
					(te = p('li')),
					(ye = p('strong')),
					_(P.$$.fragment),
					(We = u(
						': agrees with (59%) and supports (58%) the x-risk statement. Disagreement (26%) and opposition (22%) were relatively low, and sizable proportions of people remained neutral (12% and 18% for agreement and support formats, respectively).'
					)),
					(Xe = x()),
					(ne = p('li')),
					(ke = p('strong')),
					_(q.$$.fragment),
					(Ze = u(
						': 80% think AI risk is a global priority, 64% want the government to focus on catastrophic outcomes (compared to only 25% on job loss, or 5% on bias).'
					)),
					(Le = x()),
					(K = p('h2')),
					(K.textContent = kt),
					(Ue = x()),
					(d = p('ul')),
					(oe = p('li')),
					_(j.$$.fragment),
					(et = u(': 50% support a pause, 25% oppose a pause.')),
					(tt = x()),
					(se = p('li')),
					_(F.$$.fragment),
					(nt = u(
						': 72% want AI to slow down, 8% want to speed up. 83% of voters believe AI could accidentally cause a catastrophic event'
					)),
					(ot = x()),
					(ae = p('li')),
					_(Y.$$.fragment),
					(st = u(
						': 73% believe AI companies should be held liable for harms from technology they create, 67% think the AI models’ power should be restricted, and 65% believe keeping AI out of the hands of bad actors is more important than providing AI’s benefits to everyone.'
					)),
					(at = x()),
					(re = p('li')),
					_(D.$$.fragment),
					(rt = u(
						': 49:20 support “an international treaty to ban any ‘smarter-than-human’ artificial intelligence (AI)?”, 70:14 support “Preventing AI from quickly reaching superhuman capabilities”'
					)),
					(lt = x()),
					(le = p('li')),
					_(H.$$.fragment),
					(it =
						u(`: About 1 in 5 predicted AI will “definitely” stay in human control. The rest were split between those saying AI will “probably” or “definitely” get out of human control and those saying “probably not”.
Just 1 in 6 said AI shouldn’t or can’t be regulated. Only a handful trust the private sector to self-regulate.`)),
					(pt = x()),
					(ie = p('li')),
					_(J.$$.fragment),
					(ft = u(
						': There was broad support for steps that could be taken to slow down development. People supported public campaigns to slow down AI development (71.3%), government regulation that slows down development (71.0%), and a six-month pause on some kinds of AI developments (69.1%). Support for a ban on artificial general intelligence (AGI) that is smarter than humans was 62.9%.'
					)),
					(ct = x()),
					(pe = p('li')),
					_(M.$$.fragment),
					(ut = u(
						': 74% believe the government should prevent superhuman AI from quickly being created. Over 60% support a treaty with a global ban on superintelligence.'
					)),
					(mt = x()),
					(fe = p('li')),
					_(Q.$$.fragment),
					(ht = u(
						': 83% of people said that governments should require AI companies to prove their AI models are safe before releasing them.'
					)),
					($t = x()),
					(ce = p('li')),
					_(B.$$.fragment),
					(dt = u(
						': public awareness of existential risk grew in the US from 7% to 15%, and in the Netherlands and the UK 19%. Support for a government-mandated AI pause has risen in the US from 56% to 66%.'
					)),
					this.h()
			},
			l(o) {
				;(e = f(o, 'H2', { id: !0, 'data-svelte-h': !0 })),
					E(e) !== 'svelte-1ksrge5' && (e.textContent = t),
					(n = y(o)),
					(i = f(o, 'UL', {}))
				var s = c(i)
				$ = f(s, 'LI', {})
				var ue = c($)
				S = f(ue, 'STRONG', {})
				var Se = c(S)
				v(C.$$.fragment, Se),
					Se.forEach(r),
					(Pe = m(
						ue,
						': give “really bad outcomes (such as human extinction)” a 14% probability, with a median of 5%. 82% believe the control problem is important.'
					)),
					ue.forEach(r),
					(qe = y(s)),
					(W = f(s, 'LI', {}))
				var me = c(W)
				we = f(me, 'STRONG', {})
				var Ce = c(we)
				v(O.$$.fragment, Ce),
					Ce.forEach(r),
					(Ke = m(
						me,
						': Average p(doom) between 14 and 19.4%, depending on how the question of phrased. 86% believe the control problem is important.'
					)),
					me.forEach(r),
					(je = y(s)),
					(G = f(s, 'LI', {}))
				var V = c(G)
				be = f(V, 'STRONG', {})
				var Ee = c(be)
				v(T.$$.fragment, Ee),
					Ee.forEach(r),
					(Fe = m(V, ': over 60% have a ')),
					v(z.$$.fragment, V),
					(Ye = m(V, ' > 25%. Only 12% have a p(doom) = 0.')),
					V.forEach(r),
					(De = y(s)),
					(X = f(s, 'LI', {}))
				var he = c(X)
				Ie = f(he, 'STRONG', {})
				var Re = c(Ie)
				v(L.$$.fragment, Re),
					Re.forEach(r),
					(He = m(
						he,
						': respondents assigned a median probability of 20% to x-risk caused due to a lack of enough technical research, and 30% to x-risk caused due to a failure of AI systems to do what the people deploying them intended, with huge variation (for example, there are data points at both ~1% and ~99%).'
					)),
					he.forEach(r),
					(Je = y(s)),
					(Z = f(s, 'LI', {}))
				var $e = c(Z)
				Ae = f($e, 'STRONG', {})
				var Oe = c(Ae)
				v(U.$$.fragment, Oe),
					Oe.forEach(r),
					(Me = m(
						$e,
						': think there’s a 9% probability humans will go extinct because of AI. About 50% of say they’re very or somewhat worried about this.'
					)),
					$e.forEach(r),
					(Qe = y(s)),
					(ee = f(s, 'LI', {}))
				var de = c(ee)
				xe = f(de, 'STRONG', {})
				var Ge = c(xe)
				v(N.$$.fragment, Ge),
					Ge.forEach(r),
					(Be = m(
						de,
						': Only 14% believe AI will have a positive influence on the world, 40% mixed, 40% negative.'
					)),
					de.forEach(r),
					(Ve = y(s)),
					(te = f(s, 'LI', {}))
				var ge = c(te)
				ye = f(ge, 'STRONG', {})
				var Te = c(ye)
				v(P.$$.fragment, Te),
					Te.forEach(r),
					(We = m(
						ge,
						': agrees with (59%) and supports (58%) the x-risk statement. Disagreement (26%) and opposition (22%) were relatively low, and sizable proportions of people remained neutral (12% and 18% for agreement and support formats, respectively).'
					)),
					ge.forEach(r),
					(Xe = y(s)),
					(ne = f(s, 'LI', {}))
				var _e = c(ne)
				ke = f(_e, 'STRONG', {})
				var ze = c(ke)
				v(q.$$.fragment, ze),
					ze.forEach(r),
					(Ze = m(
						_e,
						': 80% think AI risk is a global priority, 64% want the government to focus on catastrophic outcomes (compared to only 25% on job loss, or 5% on bias).'
					)),
					_e.forEach(r),
					s.forEach(r),
					(Le = y(o)),
					(K = f(o, 'H2', { id: !0, 'data-svelte-h': !0 })),
					E(K) !== 'svelte-1dagog8' && (K.textContent = kt),
					(Ue = y(o)),
					(d = f(o, 'UL', {}))
				var g = c(d)
				oe = f(g, 'LI', {})
				var ve = c(oe)
				v(j.$$.fragment, ve),
					(et = m(ve, ': 50% support a pause, 25% oppose a pause.')),
					ve.forEach(r),
					(tt = y(g)),
					(se = f(g, 'LI', {}))
				var gt = c(se)
				v(F.$$.fragment, gt),
					(nt = m(
						gt,
						': 72% want AI to slow down, 8% want to speed up. 83% of voters believe AI could accidentally cause a catastrophic event'
					)),
					gt.forEach(r),
					(ot = y(g)),
					(ae = f(g, 'LI', {}))
				var _t = c(ae)
				v(Y.$$.fragment, _t),
					(st = m(
						_t,
						': 73% believe AI companies should be held liable for harms from technology they create, 67% think the AI models’ power should be restricted, and 65% believe keeping AI out of the hands of bad actors is more important than providing AI’s benefits to everyone.'
					)),
					_t.forEach(r),
					(at = y(g)),
					(re = f(g, 'LI', {}))
				var vt = c(re)
				v(D.$$.fragment, vt),
					(rt = m(
						vt,
						': 49:20 support “an international treaty to ban any ‘smarter-than-human’ artificial intelligence (AI)?”, 70:14 support “Preventing AI from quickly reaching superhuman capabilities”'
					)),
					vt.forEach(r),
					(lt = y(g)),
					(le = f(g, 'LI', {}))
				var wt = c(le)
				v(H.$$.fragment, wt),
					(it = m(
						wt,
						`: About 1 in 5 predicted AI will “definitely” stay in human control. The rest were split between those saying AI will “probably” or “definitely” get out of human control and those saying “probably not”.
Just 1 in 6 said AI shouldn’t or can’t be regulated. Only a handful trust the private sector to self-regulate.`
					)),
					wt.forEach(r),
					(pt = y(g)),
					(ie = f(g, 'LI', {}))
				var bt = c(ie)
				v(J.$$.fragment, bt),
					(ft = m(
						bt,
						': There was broad support for steps that could be taken to slow down development. People supported public campaigns to slow down AI development (71.3%), government regulation that slows down development (71.0%), and a six-month pause on some kinds of AI developments (69.1%). Support for a ban on artificial general intelligence (AGI) that is smarter than humans was 62.9%.'
					)),
					bt.forEach(r),
					(ct = y(g)),
					(pe = f(g, 'LI', {}))
				var It = c(pe)
				v(M.$$.fragment, It),
					(ut = m(
						It,
						': 74% believe the government should prevent superhuman AI from quickly being created. Over 60% support a treaty with a global ban on superintelligence.'
					)),
					It.forEach(r),
					(mt = y(g)),
					(fe = f(g, 'LI', {}))
				var At = c(fe)
				v(Q.$$.fragment, At),
					(ht = m(
						At,
						': 83% of people said that governments should require AI companies to prove their AI models are safe before releasing them.'
					)),
					At.forEach(r),
					($t = y(g)),
					(ce = f(g, 'LI', {}))
				var xt = c(ce)
				v(B.$$.fragment, xt),
					(dt = m(
						xt,
						': public awareness of existential risk grew in the US from 7% to 15%, and in the Netherlands and the UK 19%. Support for a government-mandated AI pause has risen in the US from 56% to 66%.'
					)),
					xt.forEach(r),
					g.forEach(r),
					this.h()
			},
			h() {
				Ct(e, 'id', 'catastrophic-risks-from-ai'), Ct(K, 'id', 'regulations--governance')
			},
			m(o, s) {
				h(o, e, s),
					h(o, n, s),
					h(o, i, s),
					a(i, $),
					a($, S),
					w(C, S, null),
					a($, Pe),
					a(i, qe),
					a(i, W),
					a(W, we),
					w(O, we, null),
					a(W, Ke),
					a(i, je),
					a(i, G),
					a(G, be),
					w(T, be, null),
					a(G, Fe),
					w(z, G, null),
					a(G, Ye),
					a(i, De),
					a(i, X),
					a(X, Ie),
					w(L, Ie, null),
					a(X, He),
					a(i, Je),
					a(i, Z),
					a(Z, Ae),
					w(U, Ae, null),
					a(Z, Me),
					a(i, Qe),
					a(i, ee),
					a(ee, xe),
					w(N, xe, null),
					a(ee, Be),
					a(i, Ve),
					a(i, te),
					a(te, ye),
					w(P, ye, null),
					a(te, We),
					a(i, Xe),
					a(i, ne),
					a(ne, ke),
					w(q, ke, null),
					a(ne, Ze),
					h(o, Le, s),
					h(o, K, s),
					h(o, Ue, s),
					h(o, d, s),
					a(d, oe),
					w(j, oe, null),
					a(oe, et),
					a(d, tt),
					a(d, se),
					w(F, se, null),
					a(se, nt),
					a(d, ot),
					a(d, ae),
					w(Y, ae, null),
					a(ae, st),
					a(d, at),
					a(d, re),
					w(D, re, null),
					a(re, rt),
					a(d, lt),
					a(d, le),
					w(H, le, null),
					a(le, it),
					a(d, pt),
					a(d, ie),
					w(J, ie, null),
					a(ie, ft),
					a(d, ct),
					a(d, pe),
					w(M, pe, null),
					a(pe, ut),
					a(d, mt),
					a(d, fe),
					w(Q, fe, null),
					a(fe, ht),
					a(d, $t),
					a(d, ce),
					w(B, ce, null),
					a(ce, dt),
					(Ne = !0)
			},
			p(o, s) {
				const ue = {}
				s & 2 && (ue.$$scope = { dirty: s, ctx: o }), C.$set(ue)
				const Se = {}
				s & 2 && (Se.$$scope = { dirty: s, ctx: o }), O.$set(Se)
				const me = {}
				s & 2 && (me.$$scope = { dirty: s, ctx: o }), T.$set(me)
				const Ce = {}
				s & 2 && (Ce.$$scope = { dirty: s, ctx: o }), z.$set(Ce)
				const V = {}
				s & 2 && (V.$$scope = { dirty: s, ctx: o }), L.$set(V)
				const Ee = {}
				s & 2 && (Ee.$$scope = { dirty: s, ctx: o }), U.$set(Ee)
				const he = {}
				s & 2 && (he.$$scope = { dirty: s, ctx: o }), N.$set(he)
				const Re = {}
				s & 2 && (Re.$$scope = { dirty: s, ctx: o }), P.$set(Re)
				const $e = {}
				s & 2 && ($e.$$scope = { dirty: s, ctx: o }), q.$set($e)
				const Oe = {}
				s & 2 && (Oe.$$scope = { dirty: s, ctx: o }), j.$set(Oe)
				const de = {}
				s & 2 && (de.$$scope = { dirty: s, ctx: o }), F.$set(de)
				const Ge = {}
				s & 2 && (Ge.$$scope = { dirty: s, ctx: o }), Y.$set(Ge)
				const ge = {}
				s & 2 && (ge.$$scope = { dirty: s, ctx: o }), D.$set(ge)
				const Te = {}
				s & 2 && (Te.$$scope = { dirty: s, ctx: o }), H.$set(Te)
				const _e = {}
				s & 2 && (_e.$$scope = { dirty: s, ctx: o }), J.$set(_e)
				const ze = {}
				s & 2 && (ze.$$scope = { dirty: s, ctx: o }), M.$set(ze)
				const g = {}
				s & 2 && (g.$$scope = { dirty: s, ctx: o }), Q.$set(g)
				const ve = {}
				s & 2 && (ve.$$scope = { dirty: s, ctx: o }), B.$set(ve)
			},
			i(o) {
				Ne ||
					(b(C.$$.fragment, o),
					b(O.$$.fragment, o),
					b(T.$$.fragment, o),
					b(z.$$.fragment, o),
					b(L.$$.fragment, o),
					b(U.$$.fragment, o),
					b(N.$$.fragment, o),
					b(P.$$.fragment, o),
					b(q.$$.fragment, o),
					b(j.$$.fragment, o),
					b(F.$$.fragment, o),
					b(Y.$$.fragment, o),
					b(D.$$.fragment, o),
					b(H.$$.fragment, o),
					b(J.$$.fragment, o),
					b(M.$$.fragment, o),
					b(Q.$$.fragment, o),
					b(B.$$.fragment, o),
					(Ne = !0))
			},
			o(o) {
				I(C.$$.fragment, o),
					I(O.$$.fragment, o),
					I(T.$$.fragment, o),
					I(z.$$.fragment, o),
					I(L.$$.fragment, o),
					I(U.$$.fragment, o),
					I(N.$$.fragment, o),
					I(P.$$.fragment, o),
					I(q.$$.fragment, o),
					I(j.$$.fragment, o),
					I(F.$$.fragment, o),
					I(Y.$$.fragment, o),
					I(D.$$.fragment, o),
					I(H.$$.fragment, o),
					I(J.$$.fragment, o),
					I(M.$$.fragment, o),
					I(Q.$$.fragment, o),
					I(B.$$.fragment, o),
					(Ne = !1)
			},
			d(o) {
				o && (r(e), r(n), r(i), r(Le), r(K), r(Ue), r(d)),
					A(C),
					A(O),
					A(T),
					A(z),
					A(L),
					A(U),
					A(N),
					A(P),
					A(q),
					A(j),
					A(F),
					A(Y),
					A(D),
					A(H),
					A(J),
					A(M),
					A(Q),
					A(B)
			}
		}
	)
}
function tn(l) {
	let e, t
	const n = [l[0], Rt]
	let i = { $$slots: { default: [en] }, $$scope: { ctx: l } }
	for (let $ = 0; $ < n.length; $ += 1) i = yt(i, n[$])
	return (
		(e = new Lt({ props: i })),
		{
			c() {
				_(e.$$.fragment)
			},
			l($) {
				v(e.$$.fragment, $)
			},
			m($, S) {
				w(e, $, S), (t = !0)
			},
			p($, [S]) {
				const C = S & 1 ? zt(n, [S & 1 && Et($[0]), S & 0 && Et(Rt)]) : {}
				S & 2 && (C.$$scope = { dirty: S, ctx: $ }), e.$set(C)
			},
			i($) {
				t || (b(e.$$.fragment, $), (t = !0))
			},
			o($) {
				I(e.$$.fragment, $), (t = !1)
			},
			d($) {
				A(e, $)
			}
		}
	)
}
const Rt = {
	title: 'Polls & surveys',
	description: 'How much do regular people and experts worry about AI risks and governance?'
}
function nn(l, e, t) {
	return (
		(l.$$set = (n) => {
			t(0, (e = yt(yt({}, e), St(n))))
		}),
		(e = St(e)),
		[e]
	)
}
class pn extends Gt {
	constructor(e) {
		super(), Tt(this, e, nn, tn, Ot, {})
	}
}
export { pn as default, Rt as metadata }
