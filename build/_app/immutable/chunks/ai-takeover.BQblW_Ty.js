import {
	s as cn,
	a as It,
	e as un,
	c as o,
	b as r,
	t as f,
	f as s,
	g as h,
	d as u,
	h as he,
	i as v,
	j as n,
	k as $e,
	l as a,
	m
} from './scheduler.D9JQr37X.js'
import {
	S as pn,
	i as fn,
	c as me,
	b as ce,
	m as pe,
	a as fe,
	t as ve,
	d as de
} from './index.D-WnFt3a.js'
import { g as vn, a as hn } from './a.svelte_svelte_type_style_lang.DfavE63L.js'
import { M as dn } from './mdsvex.Bi9EMyuJ.js'
import { A as Ce } from './a.YKMG9Usu.js'
function gn(d) {
	let i
	return {
		c() {
			i = f('an other article')
		},
		l(l) {
			i = v(l, 'an other article')
		},
		m(l, c) {
			a(l, i, c)
		},
		d(l) {
			l && n(i)
		}
	}
}
function yn(d) {
	let i
	return {
		c() {
			i = f('state-of-the-art AI models')
		},
		l(l) {
			i = v(l, 'state-of-the-art AI models')
		},
		m(l, c) {
			a(l, i, c)
		},
		d(l) {
			l && n(i)
		}
	}
}
function wn(d) {
	let i
	return {
		c() {
			i = f('might happen next month')
		},
		l(l) {
			i = v(l, 'might happen next month')
		},
		m(l, c) {
			a(l, i, c)
		},
		d(l) {
			l && n(i)
		}
	}
}
function bn(d) {
	let i
	return {
		c() {
			i = f('novel zero-day exploits')
		},
		l(l) {
			i = v(l, 'novel zero-day exploits')
		},
		m(l, c) {
			a(l, i, c)
		},
		d(l) {
			l && n(i)
		}
	}
}
function xn(d) {
	let i
	return {
		c() {
			i = f('“pivotal act”')
		},
		l(l) {
			i = v(l, '“pivotal act”')
		},
		m(l, c) {
			a(l, i, c)
		},
		d(l) {
			l && n(i)
		}
	}
}
function kn(d) {
	let i
	return {
		c() {
			i = f('let’s not build a superintelligence')
		},
		l(l) {
			i = v(l, 'let’s not build a superintelligence')
		},
		m(l, c) {
			a(l, i, c)
		},
		d(l) {
			l && n(i)
		}
	}
}
function _n(d) {
	let i,
		l = `One of the concerns of AI scientists is that a superintelligence could take over control of our planet.
This does not necessarily mean that everyone dies, but it does mean that (almost) all humans will lose control over our future.`,
		c,
		b,
		p,
		g,
		ge,
		ye,
		$t = 'if we build a superintelligence',
		rt,
		Se,
		_,
		Ct = 'The argument',
		Te,
		W,
		St =
			'<li>An Agentic Superintelligence is likely to exist in the (near) future.</li> <li>Some instance of the ASI will attempt a takeover.</li> <li>A takeover attempt by an ASI is likely to succeed.</li> <li>A successful takeover is permanent.</li> <li>A takeover is probably bad for most humans.</li>',
		He,
		A,
		Tt = 'An Agentic SuperIntelligence is likely to exist in the near future',
		Le,
		I,
		ut,
		$,
		ht,
		Me,
		F,
		Ht = `Not all AI systems are agents.
An agent an entity that is capable of making decisions and taking actions to achieve a goal.
A large language model, for example, does not pursue any objective on its own.
However, runtime environments can easily turn a non-agentic AI into an agentic AI.
An example of this is AutoGPT, which recursively lets a language model generate its next input.
If an SI pursues an objective in the real world, we call it an Agentic SuperIntelligence (ASI).
Since we can already turn non-agentic AI into agentic AI, we can expect that an ASI will exist shortly after an SI exists.`,
		Pe,
		C,
		mt,
		S,
		ct,
		je,
		T,
		Lt = 'Some instance of the ASI will attempt a takeover',
		qe,
		G,
		Mt = `In a takeover attempt, an ASI will take actions to maximize its control over the world.
A takeover attempt could happen for at least two reasons:`,
		ze,
		N,
		Pt =
			'<li>Because an AI is explicitly instructed to do so.</li> <li>As a sub-goal of another goal.</li>',
		Ee,
		U,
		jt =
			'This first reason is likely to happen at some point if we wait long enough, but the second reason is quite likely to happen accidentally, even early on after the creation of an ASI.',
		Oe,
		B,
		qt =
			'The sub-goal of <em>maximizing control</em> over the world could be likely to occur due to <em>instrumental convergence</em>: the tendency of sub-goals to converge on power-grabbing, self-preservation, and resource acquisition:',
		We,
		R,
		zt =
			'<li>The more control you have, the harder it will be from any other agent to prevent you from achieving your goal.</li> <li>The more control you have, the more resources you have to achieve your goal. (For example, an AI tasked with calculating pi might conclude that it would be beneficial to use all computers on the world to calculate pi.</li>',
		Fe,
		D,
		Et = `Not every instance of an ASI will necessarily attempt a takeover.
The important insight is that <strong>it only has to happen once</strong>.`,
		Ge,
		J,
		Ot = `A world which is not yet taken over, but does have an ASI that <em>could</em> take over, is in a fundamentally unstable condition.
In a similar way, a country without a government is in a fundamentally unstable condition.
It is not a question of <em>if</em> a takeover attempt will happen, but <em>when</em> it will happen.`,
		Ne,
		K,
		Wt = `The process of taking over can involve hacking into virtually all systems that are connected to the internet, manipulating people, and controlling physical resources.
A takeover attempt is successful when the ASI has control over virtually every aspect of our world.
This could be a slow process, where the ASI gradually gains more and more control over the course of months, or it could be a sudden process.
The speed at which a takeover attempt takes place will depend on the capabilities of the ASI.`,
		Ue,
		Q,
		Ft = `When an ASI has control over the world, it can prevent other ASIs from taking over.
A takeover can therefore happen only once.
A rational ASI will therefore attempt a takeover as soon as it is capable of doing so.
It is likely that the first ASI that is capable of doing so will attempt a takeover.`,
		Be,
		H,
		Gt = 'A takeover attempt by an ASI is likely to succeed',
		Re,
		V,
		Nt = `For a human, doing a takeover is an almost impossible task.
Not a single person ever successfully took over control over the entire world.
Some dictators came close, but they never had control over everything.`,
		De,
		X,
		Ut =
			'An AI has certain important advantages over humans that make a takeover attempt much more likely to succeed.',
		Je,
		Y,
		Bt =
			'<li><strong>Intelligence</strong>. A superintelligence is much smarter than a human, so it will be able to come up with better strategies to achieve its goals.</li> <li><strong>Speed</strong>. The human brain runs at 1-100hz, whereas computer chips can run at clock speeds in the GHz range.</li> <li><strong>Parallelism</strong>. A human can only do one thing at a time, whereas an AI can create new instances of itself and run them in parallel.</li> <li><strong>Memory</strong>. A human can only remember a limited amount of information, whereas an AI can store virtually unlimited amounts of information.</li> <li><strong>Collaboration</strong>. Humans can work together but are limited in the speed at which they communicate. They also have different, conflicting goals that make collaboration less effective. An AI can collaborate with other instances of itself at the speed of light, and it can have a single goal.</li> <li><strong>Self-improvement</strong>. An AI is just data and code. A sufficiently powerful AI could improve itself by writing better training algorithms, coming up with novel architectures, innovating agent runtimes or simply by scaling the amount of compute used.</li> <li><strong>Physical limitations</strong>. An AI can run on any computer, whereas humans are limited by their own physical bodies that require specific temperatures, food, water, and oxygen. Humans need to sleep and are vulnerable to diseases. An AI can use any robotic body to interact with the physical world.</li>',
		Ke,
		Z,
		Rt =
			'These various advantages will make it very unlikely that humans will be able to stop a takeover attempt.',
		Qe,
		ee,
		Dt = `One may think that an AI, as a piece of software, is still fundamentally limited by what it can do in the real world.
However, due to our reliance on the internet, AI can have a huge impact on the real world without ever leaving the digital realm.
Here are some of the things that an AI might do to take over:`,
		Ve,
		y,
		we,
		Jt =
			'<strong>Replicate itself</strong> (or just its assignment) on other computers, for example by creating accounts on free cloud providers.',
		pt,
		be,
		Kt =
			'<strong>Make money</strong> (e.g. by building a software service) and hire people to do things to help it take over.',
		ft,
		xe,
		Qt =
			'<strong>Threaten humans</strong> (with violence or blackmail) to do things that help it take over.',
		vt,
		L,
		ke,
		Vt = 'Write a virus',
		dt,
		M,
		gt,
		yt,
		_e,
		Xt =
			'Help existing robotics companies <strong>build robots</strong> (e.g. by leaking new designs), and take over these robots when they are ready.',
		wt,
		Ae,
		Yt =
			'Use existing factories to <strong>build technology that we do not understand</strong>, perhaps even using derived knowledge of physics that we are not aware of. Think nanotechnology, quantum computing, and bioengineering.',
		Xe,
		te,
		Zt = `One might suggest that having <strong>safeguards</strong> in place could prevent a takeover like adding a kill switch, or not connecting it to the internet.
However, since the ASI will be more intelligent than humans, it is likely to come up with strategies to circumvent these safeguards.
For example, if the ASI is not connected to the internet, it might manipulate humans into connecting it to the internet, for example by promising to solve a difficult problem and by behaving in a way that makes it seem trustworthy, or by convincing humans that it is a moral agent.
It’s worth noting that even this obvious safeguard is not being used as of now, as tools like ChatGPT are already connected to the internet and thousands of APIs.`,
		Ye,
		x,
		bt,
		Ie,
		en = 'prevent',
		xt,
		P,
		kt,
		Ze,
		j,
		tn = 'A takeover is probably bad for most humans',
		et,
		ne,
		nn = `The ASI that happens to take over could do so for many reasons.
For most random goals that it could have to do so, humans are not part of it.
If we end up with an ASI that is indifferent to humans, we are competing for the same resources.`,
		tt,
		ae,
		an =
			'It seems unlikely that the ASI wants to kill humanity for the sake of killing humanity - it is far more likely that it wants to use the resources that we use for some other objective. Additionally, humanity might pose a threat to the ASI’s objective, as there is a risk that we will try to stop it from achieving its goal (e.g. by turning it off).',
		nt,
		ie,
		ln = 'One of the most likely outcomes of a takeover is therefore that all humans die.',
		at,
		le,
		on = `But even in the outcomes where humans do survive, we are still at risk of being worse off.
If a goal does involve keeping humans alive, it is possible that <em>human well-being</em> is <em>not</em> part of the same goal.
It doesn’t take a lot of imagination to see how horrible it would be to be kept alive in a world where we are artificially kept alive by an ASI that is indifferent to our suffering.`,
		it,
		oe,
		sn = `And even if the AI that takes over is under human control, we don’t know that the one controlling the AI will have everyone’s best interests in mind.
It is hard to imagine a functioning democracy when an ASI exists that can manipulate people at super-human level.`,
		lt,
		q,
		rn = 'Conclusion',
		ot,
		z,
		_t,
		E,
		At,
		st
	return (
		(g = new Ce({ props: { href: '/xrisk', $$slots: { default: [gn] }, $$scope: { ctx: d } } })),
		($ = new Ce({ props: { href: '/sota', $$slots: { default: [yn] }, $$scope: { ctx: d } } })),
		(S = new Ce({ props: { href: '/urgency', $$slots: { default: [wn] }, $$scope: { ctx: d } } })),
		(M = new Ce({
			props: { href: '/cybersecurity-risks', $$slots: { default: [bn] }, $$scope: { ctx: d } }
		})),
		(P = new Ce({
			props: {
				href: 'https://arbital.com/p/pivotal/',
				rel: 'nofollow',
				$$slots: { default: [xn] },
				$$scope: { ctx: d }
			}
		})),
		(E = new Ce({ props: { href: '/action', $$slots: { default: [kn] }, $$scope: { ctx: d } } })),
		{
			c() {
				;(i = o('p')),
					(i.textContent = l),
					(c = r()),
					(b = o('p')),
					(p = f('We discuss the basics of x-risk mostly in ')),
					me(g.$$.fragment),
					(ge = f(`.
In this article here, we will argue that this takeover risk is not only real but that it is very likely to happen `)),
					(ye = o('em')),
					(ye.textContent = $t),
					(rt = f('.')),
					(Se = r()),
					(_ = o('h2')),
					(_.textContent = Ct),
					(Te = r()),
					(W = o('ul')),
					(W.innerHTML = St),
					(He = r()),
					(A = o('h2')),
					(A.textContent = Tt),
					(Le = r()),
					(I = o('p')),
					(ut =
						f(`A SuperIntelligence (SI) is a type of AI that has capabilities that surpass those of all humans in virtually every domain.
Some `)),
					me($.$$.fragment),
					(ht =
						f(` already have superhuman capabilities in certain domains, but none of them exceeds all humans at a wide range of tasks.
As AI capabilities improve due to innovations in training architectures, runtime environments, and larger scale, we can expect that an AI will eventually surpass humans in virtually every domain.`)),
					(Me = r()),
					(F = o('p')),
					(F.textContent = Ht),
					(Pe = r()),
					(C = o('p')),
					(mt = f(`It is virtually impossible to accurately predict when ASI will exist.
It might take decades, it `)),
					me(S.$$.fragment),
					(ct = f(`.
We should act as if it will happen soon, because the consequences of being wrong are so severe.`)),
					(je = r()),
					(T = o('h2')),
					(T.textContent = Lt),
					(qe = r()),
					(G = o('p')),
					(G.textContent = Mt),
					(ze = r()),
					(N = o('ol')),
					(N.innerHTML = Pt),
					(Ee = r()),
					(U = o('p')),
					(U.textContent = jt),
					(Oe = r()),
					(B = o('p')),
					(B.innerHTML = qt),
					(We = r()),
					(R = o('ul')),
					(R.innerHTML = zt),
					(Fe = r()),
					(D = o('p')),
					(D.innerHTML = Et),
					(Ge = r()),
					(J = o('p')),
					(J.innerHTML = Ot),
					(Ne = r()),
					(K = o('p')),
					(K.textContent = Wt),
					(Ue = r()),
					(Q = o('p')),
					(Q.textContent = Ft),
					(Be = r()),
					(H = o('h2')),
					(H.textContent = Gt),
					(Re = r()),
					(V = o('p')),
					(V.textContent = Nt),
					(De = r()),
					(X = o('p')),
					(X.textContent = Ut),
					(Je = r()),
					(Y = o('ol')),
					(Y.innerHTML = Bt),
					(Ke = r()),
					(Z = o('p')),
					(Z.textContent = Rt),
					(Qe = r()),
					(ee = o('p')),
					(ee.textContent = Dt),
					(Ve = r()),
					(y = o('ul')),
					(we = o('li')),
					(we.innerHTML = Jt),
					(pt = r()),
					(be = o('li')),
					(be.innerHTML = Kt),
					(ft = r()),
					(xe = o('li')),
					(xe.innerHTML = Qt),
					(vt = r()),
					(L = o('li')),
					(ke = o('strong')),
					(ke.textContent = Vt),
					(dt = f(' using ')),
					me(M.$$.fragment),
					(gt = f(' that infects other computers, replicating itself or gaining full control.')),
					(yt = r()),
					(_e = o('li')),
					(_e.innerHTML = Xt),
					(wt = r()),
					(Ae = o('li')),
					(Ae.innerHTML = Yt),
					(Xe = r()),
					(te = o('p')),
					(te.innerHTML = Zt),
					(Ye = r()),
					(x = o('p')),
					(bt = f('One other solution is to use ASI to ')),
					(Ie = o('em')),
					(Ie.textContent = en),
					(xt = f(` a takeover.
An aligned ASI would be able to come up with strategies that prevent other ASIs from taking over.
This is sometimes referred to as a `)),
					me(P.$$.fragment),
					(kt = f('.')),
					(Ze = r()),
					(j = o('h2')),
					(j.textContent = tn),
					(et = r()),
					(ne = o('p')),
					(ne.textContent = nn),
					(tt = r()),
					(ae = o('p')),
					(ae.textContent = an),
					(nt = r()),
					(ie = o('p')),
					(ie.textContent = ln),
					(at = r()),
					(le = o('p')),
					(le.innerHTML = on),
					(it = r()),
					(oe = o('p')),
					(oe.textContent = sn),
					(lt = r()),
					(q = o('h2')),
					(q.textContent = rn),
					(ot = r()),
					(z = o('p')),
					(_t =
						f(`If these premises are true, then the likelihood of an AI takeover approaches certainty as AI surpasses human capabilities.
So `)),
					me(E.$$.fragment),
					(At = f('.')),
					this.h()
			},
			l(e) {
				;(i = s(e, 'P', { 'data-svelte-h': !0 })),
					h(i) !== 'svelte-1bfzno1' && (i.textContent = l),
					(c = u(e)),
					(b = s(e, 'P', {}))
				var t = he(b)
				;(p = v(t, 'We discuss the basics of x-risk mostly in ')),
					ce(g.$$.fragment, t),
					(ge = v(
						t,
						`.
In this article here, we will argue that this takeover risk is not only real but that it is very likely to happen `
					)),
					(ye = s(t, 'EM', { 'data-svelte-h': !0 })),
					h(ye) !== 'svelte-11p6740' && (ye.textContent = $t),
					(rt = v(t, '.')),
					t.forEach(n),
					(Se = u(e)),
					(_ = s(e, 'H2', { id: !0, 'data-svelte-h': !0 })),
					h(_) !== 'svelte-diclnd' && (_.textContent = Ct),
					(Te = u(e)),
					(W = s(e, 'UL', { 'data-svelte-h': !0 })),
					h(W) !== 'svelte-kxs44j' && (W.innerHTML = St),
					(He = u(e)),
					(A = s(e, 'H2', { id: !0, 'data-svelte-h': !0 })),
					h(A) !== 'svelte-13w4gg8' && (A.textContent = Tt),
					(Le = u(e)),
					(I = s(e, 'P', {}))
				var se = he(I)
				;(ut = v(
					se,
					`A SuperIntelligence (SI) is a type of AI that has capabilities that surpass those of all humans in virtually every domain.
Some `
				)),
					ce($.$$.fragment, se),
					(ht = v(
						se,
						` already have superhuman capabilities in certain domains, but none of them exceeds all humans at a wide range of tasks.
As AI capabilities improve due to innovations in training architectures, runtime environments, and larger scale, we can expect that an AI will eventually surpass humans in virtually every domain.`
					)),
					se.forEach(n),
					(Me = u(e)),
					(F = s(e, 'P', { 'data-svelte-h': !0 })),
					h(F) !== 'svelte-1r5jpzu' && (F.textContent = Ht),
					(Pe = u(e)),
					(C = s(e, 'P', {}))
				var re = he(C)
				;(mt = v(
					re,
					`It is virtually impossible to accurately predict when ASI will exist.
It might take decades, it `
				)),
					ce(S.$$.fragment, re),
					(ct = v(
						re,
						`.
We should act as if it will happen soon, because the consequences of being wrong are so severe.`
					)),
					re.forEach(n),
					(je = u(e)),
					(T = s(e, 'H2', { id: !0, 'data-svelte-h': !0 })),
					h(T) !== 'svelte-1696nj8' && (T.textContent = Lt),
					(qe = u(e)),
					(G = s(e, 'P', { 'data-svelte-h': !0 })),
					h(G) !== 'svelte-2j9rn1' && (G.textContent = Mt),
					(ze = u(e)),
					(N = s(e, 'OL', { 'data-svelte-h': !0 })),
					h(N) !== 'svelte-126693n' && (N.innerHTML = Pt),
					(Ee = u(e)),
					(U = s(e, 'P', { 'data-svelte-h': !0 })),
					h(U) !== 'svelte-1av33th' && (U.textContent = jt),
					(Oe = u(e)),
					(B = s(e, 'P', { 'data-svelte-h': !0 })),
					h(B) !== 'svelte-ks0tux' && (B.innerHTML = qt),
					(We = u(e)),
					(R = s(e, 'UL', { 'data-svelte-h': !0 })),
					h(R) !== 'svelte-1ctyb9z' && (R.innerHTML = zt),
					(Fe = u(e)),
					(D = s(e, 'P', { 'data-svelte-h': !0 })),
					h(D) !== 'svelte-1gubxpt' && (D.innerHTML = Et),
					(Ge = u(e)),
					(J = s(e, 'P', { 'data-svelte-h': !0 })),
					h(J) !== 'svelte-fqpmur' && (J.innerHTML = Ot),
					(Ne = u(e)),
					(K = s(e, 'P', { 'data-svelte-h': !0 })),
					h(K) !== 'svelte-b951ky' && (K.textContent = Wt),
					(Ue = u(e)),
					(Q = s(e, 'P', { 'data-svelte-h': !0 })),
					h(Q) !== 'svelte-1normyz' && (Q.textContent = Ft),
					(Be = u(e)),
					(H = s(e, 'H2', { id: !0, 'data-svelte-h': !0 })),
					h(H) !== 'svelte-pe5irf' && (H.textContent = Gt),
					(Re = u(e)),
					(V = s(e, 'P', { 'data-svelte-h': !0 })),
					h(V) !== 'svelte-19cohqy' && (V.textContent = Nt),
					(De = u(e)),
					(X = s(e, 'P', { 'data-svelte-h': !0 })),
					h(X) !== 'svelte-1l3d4bu' && (X.textContent = Ut),
					(Je = u(e)),
					(Y = s(e, 'OL', { 'data-svelte-h': !0 })),
					h(Y) !== 'svelte-7tyjm6' && (Y.innerHTML = Bt),
					(Ke = u(e)),
					(Z = s(e, 'P', { 'data-svelte-h': !0 })),
					h(Z) !== 'svelte-4z8ocy' && (Z.textContent = Rt),
					(Qe = u(e)),
					(ee = s(e, 'P', { 'data-svelte-h': !0 })),
					h(ee) !== 'svelte-pqpglt' && (ee.textContent = Dt),
					(Ve = u(e)),
					(y = s(e, 'UL', {}))
				var w = he(y)
				;(we = s(w, 'LI', { 'data-svelte-h': !0 })),
					h(we) !== 'svelte-cd4pxs' && (we.innerHTML = Jt),
					(pt = u(w)),
					(be = s(w, 'LI', { 'data-svelte-h': !0 })),
					h(be) !== 'svelte-1en02hv' && (be.innerHTML = Kt),
					(ft = u(w)),
					(xe = s(w, 'LI', { 'data-svelte-h': !0 })),
					h(xe) !== 'svelte-1gtn8pe' && (xe.innerHTML = Qt),
					(vt = u(w)),
					(L = s(w, 'LI', {}))
				var O = he(L)
				;(ke = s(O, 'STRONG', { 'data-svelte-h': !0 })),
					h(ke) !== 'svelte-ra5q01' && (ke.textContent = Vt),
					(dt = v(O, ' using ')),
					ce(M.$$.fragment, O),
					(gt = v(O, ' that infects other computers, replicating itself or gaining full control.')),
					O.forEach(n),
					(yt = u(w)),
					(_e = s(w, 'LI', { 'data-svelte-h': !0 })),
					h(_e) !== 'svelte-1wcxvzj' && (_e.innerHTML = Xt),
					(wt = u(w)),
					(Ae = s(w, 'LI', { 'data-svelte-h': !0 })),
					h(Ae) !== 'svelte-m55s7d' && (Ae.innerHTML = Yt),
					w.forEach(n),
					(Xe = u(e)),
					(te = s(e, 'P', { 'data-svelte-h': !0 })),
					h(te) !== 'svelte-1nkga8e' && (te.innerHTML = Zt),
					(Ye = u(e)),
					(x = s(e, 'P', {}))
				var k = he(x)
				;(bt = v(k, 'One other solution is to use ASI to ')),
					(Ie = s(k, 'EM', { 'data-svelte-h': !0 })),
					h(Ie) !== 'svelte-1gq3gn0' && (Ie.textContent = en),
					(xt = v(
						k,
						` a takeover.
An aligned ASI would be able to come up with strategies that prevent other ASIs from taking over.
This is sometimes referred to as a `
					)),
					ce(P.$$.fragment, k),
					(kt = v(k, '.')),
					k.forEach(n),
					(Ze = u(e)),
					(j = s(e, 'H2', { id: !0, 'data-svelte-h': !0 })),
					h(j) !== 'svelte-1ecabxp' && (j.textContent = tn),
					(et = u(e)),
					(ne = s(e, 'P', { 'data-svelte-h': !0 })),
					h(ne) !== 'svelte-1ld8fgv' && (ne.textContent = nn),
					(tt = u(e)),
					(ae = s(e, 'P', { 'data-svelte-h': !0 })),
					h(ae) !== 'svelte-grl48c' && (ae.textContent = an),
					(nt = u(e)),
					(ie = s(e, 'P', { 'data-svelte-h': !0 })),
					h(ie) !== 'svelte-1nf2hpr' && (ie.textContent = ln),
					(at = u(e)),
					(le = s(e, 'P', { 'data-svelte-h': !0 })),
					h(le) !== 'svelte-ek96c6' && (le.innerHTML = on),
					(it = u(e)),
					(oe = s(e, 'P', { 'data-svelte-h': !0 })),
					h(oe) !== 'svelte-un8xsj' && (oe.textContent = sn),
					(lt = u(e)),
					(q = s(e, 'H2', { id: !0, 'data-svelte-h': !0 })),
					h(q) !== 'svelte-1cykd0u' && (q.textContent = rn),
					(ot = u(e)),
					(z = s(e, 'P', {}))
				var ue = he(z)
				;(_t = v(
					ue,
					`If these premises are true, then the likelihood of an AI takeover approaches certainty as AI surpasses human capabilities.
So `
				)),
					ce(E.$$.fragment, ue),
					(At = v(ue, '.')),
					ue.forEach(n),
					this.h()
			},
			h() {
				$e(_, 'id', 'the-argument'),
					$e(A, 'id', 'an-agentic-superintelligence-is-likely-to-exist-in-the-near-future'),
					$e(T, 'id', 'some-instance-of-the-asi-will-attempt-a-takeover'),
					$e(H, 'id', 'a-takeover-attempt-by-an-asi-is-likely-to-succeed'),
					$e(j, 'id', 'a-takeover-is-probably-bad-for-most-humans'),
					$e(q, 'id', 'conclusion')
			},
			m(e, t) {
				a(e, i, t),
					a(e, c, t),
					a(e, b, t),
					m(b, p),
					pe(g, b, null),
					m(b, ge),
					m(b, ye),
					m(b, rt),
					a(e, Se, t),
					a(e, _, t),
					a(e, Te, t),
					a(e, W, t),
					a(e, He, t),
					a(e, A, t),
					a(e, Le, t),
					a(e, I, t),
					m(I, ut),
					pe($, I, null),
					m(I, ht),
					a(e, Me, t),
					a(e, F, t),
					a(e, Pe, t),
					a(e, C, t),
					m(C, mt),
					pe(S, C, null),
					m(C, ct),
					a(e, je, t),
					a(e, T, t),
					a(e, qe, t),
					a(e, G, t),
					a(e, ze, t),
					a(e, N, t),
					a(e, Ee, t),
					a(e, U, t),
					a(e, Oe, t),
					a(e, B, t),
					a(e, We, t),
					a(e, R, t),
					a(e, Fe, t),
					a(e, D, t),
					a(e, Ge, t),
					a(e, J, t),
					a(e, Ne, t),
					a(e, K, t),
					a(e, Ue, t),
					a(e, Q, t),
					a(e, Be, t),
					a(e, H, t),
					a(e, Re, t),
					a(e, V, t),
					a(e, De, t),
					a(e, X, t),
					a(e, Je, t),
					a(e, Y, t),
					a(e, Ke, t),
					a(e, Z, t),
					a(e, Qe, t),
					a(e, ee, t),
					a(e, Ve, t),
					a(e, y, t),
					m(y, we),
					m(y, pt),
					m(y, be),
					m(y, ft),
					m(y, xe),
					m(y, vt),
					m(y, L),
					m(L, ke),
					m(L, dt),
					pe(M, L, null),
					m(L, gt),
					m(y, yt),
					m(y, _e),
					m(y, wt),
					m(y, Ae),
					a(e, Xe, t),
					a(e, te, t),
					a(e, Ye, t),
					a(e, x, t),
					m(x, bt),
					m(x, Ie),
					m(x, xt),
					pe(P, x, null),
					m(x, kt),
					a(e, Ze, t),
					a(e, j, t),
					a(e, et, t),
					a(e, ne, t),
					a(e, tt, t),
					a(e, ae, t),
					a(e, nt, t),
					a(e, ie, t),
					a(e, at, t),
					a(e, le, t),
					a(e, it, t),
					a(e, oe, t),
					a(e, lt, t),
					a(e, q, t),
					a(e, ot, t),
					a(e, z, t),
					m(z, _t),
					pe(E, z, null),
					m(z, At),
					(st = !0)
			},
			p(e, t) {
				const se = {}
				t & 2 && (se.$$scope = { dirty: t, ctx: e }), g.$set(se)
				const re = {}
				t & 2 && (re.$$scope = { dirty: t, ctx: e }), $.$set(re)
				const w = {}
				t & 2 && (w.$$scope = { dirty: t, ctx: e }), S.$set(w)
				const O = {}
				t & 2 && (O.$$scope = { dirty: t, ctx: e }), M.$set(O)
				const k = {}
				t & 2 && (k.$$scope = { dirty: t, ctx: e }), P.$set(k)
				const ue = {}
				t & 2 && (ue.$$scope = { dirty: t, ctx: e }), E.$set(ue)
			},
			i(e) {
				st ||
					(fe(g.$$.fragment, e),
					fe($.$$.fragment, e),
					fe(S.$$.fragment, e),
					fe(M.$$.fragment, e),
					fe(P.$$.fragment, e),
					fe(E.$$.fragment, e),
					(st = !0))
			},
			o(e) {
				ve(g.$$.fragment, e),
					ve($.$$.fragment, e),
					ve(S.$$.fragment, e),
					ve(M.$$.fragment, e),
					ve(P.$$.fragment, e),
					ve(E.$$.fragment, e),
					(st = !1)
			},
			d(e) {
				e &&
					(n(i),
					n(c),
					n(b),
					n(Se),
					n(_),
					n(Te),
					n(W),
					n(He),
					n(A),
					n(Le),
					n(I),
					n(Me),
					n(F),
					n(Pe),
					n(C),
					n(je),
					n(T),
					n(qe),
					n(G),
					n(ze),
					n(N),
					n(Ee),
					n(U),
					n(Oe),
					n(B),
					n(We),
					n(R),
					n(Fe),
					n(D),
					n(Ge),
					n(J),
					n(Ne),
					n(K),
					n(Ue),
					n(Q),
					n(Be),
					n(H),
					n(Re),
					n(V),
					n(De),
					n(X),
					n(Je),
					n(Y),
					n(Ke),
					n(Z),
					n(Qe),
					n(ee),
					n(Ve),
					n(y),
					n(Xe),
					n(te),
					n(Ye),
					n(x),
					n(Ze),
					n(j),
					n(et),
					n(ne),
					n(tt),
					n(ae),
					n(nt),
					n(ie),
					n(at),
					n(le),
					n(it),
					n(oe),
					n(lt),
					n(q),
					n(ot),
					n(z)),
					de(g),
					de($),
					de(S),
					de(M),
					de(P),
					de(E)
			}
		}
	)
}
function An(d) {
	let i, l
	const c = [d[0], mn]
	let b = { $$slots: { default: [_n] }, $$scope: { ctx: d } }
	for (let p = 0; p < c.length; p += 1) b = It(b, c[p])
	return (
		(i = new dn({ props: b })),
		{
			c() {
				me(i.$$.fragment)
			},
			l(p) {
				ce(i.$$.fragment, p)
			},
			m(p, g) {
				pe(i, p, g), (l = !0)
			},
			p(p, [g]) {
				const ge = g & 1 ? vn(c, [g & 1 && hn(p[0]), g & 0 && hn(mn)]) : {}
				g & 2 && (ge.$$scope = { dirty: g, ctx: p }), i.$set(ge)
			},
			i(p) {
				l || (fe(i.$$.fragment, p), (l = !0))
			},
			o(p) {
				ve(i.$$.fragment, p), (l = !1)
			},
			d(p) {
				de(i, p)
			}
		}
	)
}
const mn = {
	title: 'Why an AI takeover could be very likely',
	description:
		'As AI surpasses human capabilities, the likelihood of an AI takeover becomes very high.'
}
function In(d, i, l) {
	return (
		(d.$$set = (c) => {
			l(0, (i = It(It({}, i), un(c))))
		}),
		(i = un(i)),
		[i]
	)
}
class Ln extends pn {
	constructor(i) {
		super(), fn(this, i, In, An, cn, {})
	}
}
export { Ln as default, mn as metadata }
